{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7a7b861-6c63-4c4d-8715-b81b8ab6e28c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 1: Importing Libraries and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006cd159-8e04-4f87-b730-ec2b74869bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Installed **HistorianQuery** for historian data access, **PyCaret (full)** for AutoML, and **PyTorch with vision/audio** for deep learning on CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84638080-77fa-447a-9099-c2ceb5223ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install historian_query pycaret[full]\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install lifelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf7946b-30d0-46c5-8877-082723260b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd6f82cd-4f1b-4e9d-a3fb-6ed94dc8d320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import logging\n",
    "from itertools import chain\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import window, avg, create_map, lit, col, expr, from_unixtime, to_timestamp\n",
    "from datetime import datetime, timedelta\n",
    "from functools import singledispatchmethod\n",
    "import pyspark.sql.functions as sfn\n",
    "from tempo import TSDF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0edcfe-6cdb-43b3-80c3-c6518530f753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1.1: Historian Class for Influx Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b9b8ab6-0593-4fb8-894d-f2fc4b2693c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Hhelper to pull, clean, and regularize historian time-series data on Spark into tidy, evenly sampled series.\n",
    "* **Three ways to init:**\n",
    "\n",
    "  1. from an existing DataFrame; 2) from a table + list of tags; 3) from a table + aligned triplets (tag, reference, controller).\n",
    "* **Schema expectations:** Table mode requires standard historian columns (tag/reference/controller, ts, value\\_double, site, historian\\_name, \\_event\\_date, optional line).\n",
    "* **Resampling core:** `resample(start, end, ...)` does ceil-based resampling with forward-fill, pads edges, enforces a **forward-fill timeout** (stale values → null), and returns a slim TS (optionally joined to aliases).\n",
    "* **Alias support:** Provide an alias map to enrich outputs with `alias`, while keeping reference/controller—useful for human-readable plotting and joins.\n",
    "* **Null handling:** `ignore_nulls=False` turns nulls into a sentinel to preserve “no value” vs “stale after timeout”; then it restores nulls post-resample.\n",
    "* **Safety checks:** Verifies `ff_timeout ≥ sample_freq`; filters by site/historian\\_name/date (and line when present).\n",
    "* **Utilities:** `get_raw_data(...)` gets unregularized slices; `get_latest_ts()` finds most recent timestamp; `with_alias(...)` returns **all** historian columns + alias for richer downstream use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec27917-cd00-4c44-831f-5219e2827ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import singledispatchmethod\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql import functions as sfn\n",
    "from pyspark.sql.functions import col\n",
    "from tempo import TSDF\n",
    "\n",
    "class HistorianQuery:\n",
    "    \"\"\"\n",
    "    Query & regularize time series from a historian table/DataFrame on Spark.\n",
    "\n",
    "    Modes:\n",
    "      1) DataFrame:\n",
    "         HistorianQuery(df, sample_freq, ff_timeout, site, historian_name, start_date, end_date,\n",
    "                        line: Optional[str]=None, ignore_nulls=False)\n",
    "\n",
    "      2) Table + tag list:\n",
    "         HistorianQuery(table_name: str, tag_list: list[str], sample_freq, ff_timeout,\n",
    "                        site, historian_name, start_date, end_date,\n",
    "                        line: Optional[str]=None, ignore_nulls=False)\n",
    "\n",
    "      3) Table + three arrays (tag, reference, controller):\n",
    "         HistorianQuery(table_name: str,\n",
    "                        tag_names: list[str], ref_names: list[str], ctrl_names: list[str],\n",
    "                        sample_freq, ff_timeout, site, historian_name, start_date, end_date,\n",
    "                        line: Optional[str]=None, ignore_nulls=False)\n",
    "\n",
    "    Notes:\n",
    "      - In table modes, the table must contain:\n",
    "        [\"tag_name\",\"reference_name\",\"controller_name\",\"ts\",\"value_double\",\n",
    "         \"_event_date\",\"site\",\"historian_name\"] and optionally \"line\".\n",
    "      - .resample(...) returns a slim TS (tag_name, ts, value_double, orig_ts)\n",
    "      - .with_alias(alias) returns ALL historian columns + an 'alias' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------\n",
    "    # DF mode\n",
    "    # ---------------------------\n",
    "    @singledispatchmethod\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        sample_freq: str,\n",
    "        ff_timeout: str,\n",
    "        site: str,\n",
    "        historian_name: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        line: Optional[str] = None,\n",
    "        ignore_nulls: bool = False,\n",
    "    ):\n",
    "        self.sample_freq = sample_freq\n",
    "        self.ff_timeout = ff_timeout\n",
    "        self.ignore_nulls = ignore_nulls\n",
    "        self.column_types = {\"value_double\": \"double\"}\n",
    "\n",
    "        # ✅ keep ALL columns here for alias joins later\n",
    "        self.df_full = df\n",
    "\n",
    "        # ✅ slim view used for resampling (NO 'alias' here!)\n",
    "        self.required_cols = [\"tag_name\", \"ts\", \"value_double\"]\n",
    "        self.df = df.select(*self.required_cols)\n",
    "\n",
    "        if not spark.sql(f\"select interval {ff_timeout} >= interval {sample_freq}\").first()[0]:\n",
    "            raise ValueError(\n",
    "                \"Timeout interval must be at least as long as sampling frequency, \"\n",
    "                f\"but {ff_timeout} < {sample_freq}.\"\n",
    "            )\n",
    "\n",
    "        self.table_name = None\n",
    "        self.tag_list = None\n",
    "\n",
    "    # ---------------------------\n",
    "    # Table mode (tag list OR triplets)\n",
    "    # ---------------------------\n",
    "    @__init__.register\n",
    "    def _(\n",
    "        self,\n",
    "        table_name: str,\n",
    "        list1: List[str],\n",
    "        *rest,\n",
    "        sample_freq: str,\n",
    "        ff_timeout: str,\n",
    "        site: str,\n",
    "        historian_name: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        line: Optional[str] = None,\n",
    "        ignore_nulls: bool = False,\n",
    "    ):\n",
    "        self.table_name = table_name\n",
    "        self.sample_freq = sample_freq\n",
    "        self.ff_timeout = ff_timeout\n",
    "        self.ignore_nulls = ignore_nulls\n",
    "        self.column_types = {\"value_double\": \"double\"}\n",
    "        #  slim schema for Tempo only \n",
    "        self.required_cols = [\"tag_name\", \"ts\", \"value_double\"]\n",
    "\n",
    "        if not spark.sql(f\"select interval {ff_timeout} >= interval {sample_freq}\").first()[0]:\n",
    "            raise ValueError(\n",
    "                \"Timeout interval must be at least as long as sampling frequency, \"\n",
    "                f\"but {ff_timeout} < {sample_freq}.\"\n",
    "            )\n",
    "\n",
    "        base = spark.table(table_name).where(\n",
    "            (col(\"_event_date\").between(start_date, end_date))\n",
    "            & (col(\"site\").ilike(f\"%{site}%\"))\n",
    "            & (col(\"historian_name\").ilike(f\"%{historian_name}%\"))\n",
    "        )\n",
    "        if line is not None and \"line\" in base.columns:\n",
    "            base = base.where(col(\"line\") == line)\n",
    "\n",
    "        # Decide variant\n",
    "        use_triples = len(rest) >= 2 and isinstance(rest[0], list) and isinstance(rest[1], list)\n",
    "        if use_triples:\n",
    "            tag_names = list1\n",
    "            ref_names = rest[0]\n",
    "            ctrl_names = rest[1]\n",
    "            if not (len(tag_names) == len(ref_names) == len(ctrl_names)):\n",
    "                raise ValueError(\n",
    "                    \"tag_names, ref_names, and ctrl_names must be the same length \"\n",
    "                    f\"(got {len(tag_names)}, {len(ref_names)}, {len(ctrl_names)})\"\n",
    "                )\n",
    "\n",
    "            triples_rows = [\n",
    "                Row(tag_name=t, reference_name=r, controller_name=c)\n",
    "                for t, r, c in zip(tag_names, ref_names, ctrl_names)\n",
    "            ]\n",
    "            filter_df = spark.createDataFrame(triples_rows).dropDuplicates()\n",
    "\n",
    "            # keep ALL columns, filtered to the triplets\n",
    "            cond = (\n",
    "                base[\"tag_name\"] == filter_df[\"tag_name\"]\n",
    "            ) & (\n",
    "                base[\"controller_name\"] == filter_df[\"controller_name\"]\n",
    "            ) & (\n",
    "                base[\"reference_name\"].eqNullSafe(filter_df[\"reference_name\"])\n",
    "            )\n",
    "            df_full = base.join(filter_df, cond, \"semi\")\n",
    "            self.tag_list = tag_names\n",
    "        else:\n",
    "            tag_list = list1\n",
    "            df_full = base.where(col(\"tag_name\").isin(tag_list))\n",
    "            self.tag_list = tag_list\n",
    "\n",
    "        # store both views\n",
    "        self.df_full = df_full\n",
    "        self.df = df_full.select(*self.required_cols)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Public API\n",
    "    # ---------------------------\n",
    "    \n",
    "    def resample(\n",
    "        self,\n",
    "        start_ts_str: str,\n",
    "        end_ts_str: str,\n",
    "        alias_map: Optional[Union[DataFrame, List[Tuple[str, Optional[str], str, str]]]] = None\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Regularized time series using ceil-resample + forward fill.\n",
    "        If alias_map is provided, output also includes reference_name and alias.\n",
    "        Output columns (when alias_map is given):\n",
    "        tag_name, reference_name, controller_name, alias, ts, value_double, orig_ts\n",
    "        Otherwise:\n",
    "        tag_name, ts, value_double, orig_ts\n",
    "        \"\"\"\n",
    "        # --- work off the full DF so we keep reference/controller columns\n",
    "        base = self.df_full.select(\"tag_name\", \"reference_name\", \"controller_name\", \"ts\", \"value_double\")\n",
    "\n",
    "        start_ts = self.str2ts(start_ts_str)\n",
    "        end_ts = self.str2ts(end_ts_str)\n",
    "        ff_timeout = self.ff_timeout\n",
    "        sample_freq = self.sample_freq\n",
    "        ignore_nulls = self.ignore_nulls\n",
    "        column_types = self.column_types\n",
    "\n",
    "        # raw window with lead for ffill\n",
    "        df = base.where(sfn.col(\"ts\") > start_ts - sfn.expr(f\"interval {ff_timeout}\")) \\\n",
    "                .where(sfn.col(\"ts\") < end_ts)\n",
    "\n",
    "        if ignore_nulls:\n",
    "            df = df.na.drop(subset=[\"value_double\"])\n",
    "\n",
    "        # pad early per (tag,ref,ctrl)\n",
    "        early_pad = (df\n",
    "            .select(\"tag_name\", \"reference_name\", \"controller_name\")\n",
    "            .distinct()\n",
    "            .withColumn(\"ts\", start_ts - sfn.expr(f\"interval {ff_timeout}\"))\n",
    "            .withColumn(\"value_double\", sfn.lit(None).cast(column_types[\"value_double\"]))\n",
    "        )\n",
    "        df = early_pad.unionByName(df)\n",
    "\n",
    "        # pad final per (tag,ref,ctrl)\n",
    "        late_pad = (df\n",
    "            .select(\"tag_name\", \"reference_name\", \"controller_name\")\n",
    "            .distinct()\n",
    "            .withColumn(\"ts\", end_ts)\n",
    "            .withColumn(\"value_double\", sfn.lit(None).cast(column_types[\"value_double\"]))\n",
    "        )\n",
    "        df = df.unionByName(late_pad)\n",
    "\n",
    "        # preserve write times\n",
    "        df = df.withColumn(\"orig_ts_double\", sfn.col(\"ts\").cast(\"double\"))\n",
    "\n",
    "        # shift timestamps so ceil-resample picks last value\n",
    "        df = df.withColumn(\"ts\", sfn.col(\"ts\") + sfn.expr(f\"interval {sample_freq} - interval 1 millisecond\"))\n",
    "\n",
    "        if not ignore_nulls:\n",
    "            neg_inf = sfn.lit(\"-Inf\").cast(column_types[\"value_double\"])\n",
    "            df = df.withColumn(\n",
    "                \"value_double\",\n",
    "                sfn.when(sfn.col(\"value_double\").isNull(), neg_inf).otherwise(sfn.col(\"value_double\")),\n",
    "            )\n",
    "\n",
    "        # resample by (tag_name, reference_name, controller_name)\n",
    "        df_out = (\n",
    "            TSDF(df, partition_cols=[\"tag_name\", \"reference_name\", \"controller_name\"], ts_col=\"ts\")\n",
    "            .resample(freq=sample_freq, func=\"ceil\")\n",
    "            .interpolate(method=\"ffill\", show_interpolated=False)\n",
    "            .df\n",
    "        )\n",
    "\n",
    "        if not ignore_nulls:\n",
    "            neg_inf = sfn.lit(\"-Inf\").cast(column_types[\"value_double\"])\n",
    "            df_out = df_out.withColumn(\n",
    "                \"value_double\",\n",
    "                sfn.when(sfn.col(\"value_double\") == neg_inf, sfn.lit(None).cast(\"double\"))\n",
    "                .otherwise(sfn.col(\"value_double\")),\n",
    "            )\n",
    "\n",
    "        # recover orig_ts\n",
    "        df_out = df_out.withColumn(\"orig_ts\", sfn.to_timestamp(sfn.col(\"orig_ts_double\"))).drop(\"orig_ts_double\")\n",
    "\n",
    "        # null out stale values beyond timeout\n",
    "        df_out = df_out.withColumn(\n",
    "            \"value_double\",\n",
    "            sfn.when(sfn.col(\"ts\") - sfn.col(\"orig_ts\") >= sfn.expr(\"interval \" + ff_timeout), None)\n",
    "            .otherwise(sfn.col(\"value_double\")),\n",
    "        )\n",
    "\n",
    "        # clip final window\n",
    "        df_out = df_out.filter(sfn.col(\"ts\") >= start_ts).filter(sfn.col(\"ts\") < end_ts)\n",
    "\n",
    "        # Optionally join alias to add alias column and keep reference_name\n",
    "        if alias_map is not None:\n",
    "            if isinstance(alias_map, list):\n",
    "                alias_df = spark.createDataFrame(\n",
    "                    alias_map, [\"tag_name\", \"reference_name\", \"controller_name\", \"alias\"]\n",
    "                ).dropDuplicates()\n",
    "            else:\n",
    "                alias_df = alias_map.select(\"tag_name\", \"reference_name\", \"controller_name\", \"alias\").dropDuplicates()\n",
    "\n",
    "            df_out = (df_out.alias(\"t\")\n",
    "                .join(\n",
    "                    alias_df.alias(\"m\"),\n",
    "                    (col(\"t.tag_name\") == col(\"m.tag_name\")) &\n",
    "                    (col(\"t.reference_name\").eqNullSafe(col(\"m.reference_name\"))) &\n",
    "                    (col(\"t.controller_name\") == col(\"m.controller_name\")),\n",
    "                    \"inner\"\n",
    "                )\n",
    "                .select(\n",
    "                    col(\"t.tag_name\").alias(\"tag_name\"),\n",
    "                    col(\"t.reference_name\").alias(\"reference_name\"),\n",
    "                    col(\"t.controller_name\").alias(\"controller_name\"),\n",
    "                    col(\"m.alias\").alias(\"alias\"),\n",
    "                    col(\"t.ts\").alias(\"ts\"),\n",
    "                    col(\"t.value_double\").alias(\"value_double\"),\n",
    "                    col(\"t.orig_ts\").alias(\"orig_ts\"),\n",
    "                )\n",
    "            )\n",
    "            return df_out\n",
    "\n",
    "        # default (no alias)\n",
    "        return df_out.select(\"tag_name\", \"ts\", \"value_double\", \"orig_ts\")\n",
    "\n",
    "\n",
    "    def get_raw_data(self, start_ts_str: str, end_ts_str: str) -> DataFrame:\n",
    "        start_ts = self.str2ts(start_ts_str)\n",
    "        end_ts = self.str2ts(end_ts_str)\n",
    "        ff_timeout = self.ff_timeout\n",
    "        df_raw = self.df.where(sfn.col(\"ts\") > start_ts - sfn.expr(f\"interval {ff_timeout}\"))\n",
    "        df_raw = df_raw.where(sfn.col(\"ts\") < end_ts)\n",
    "        return df_raw\n",
    "\n",
    "    def get_latest_ts(self) -> str:\n",
    "        recent = self.df.filter(self.df.ts >= sfn.date_sub(sfn.current_date(), 1))\n",
    "        max_ts = self._get_latest_ts(recent) or self._get_latest_ts(self.df)\n",
    "        if max_ts is None:\n",
    "            raise EOFError(\"No records found\")\n",
    "        return max_ts.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_latest_ts(df: DataFrame):\n",
    "        row = df.select(sfn.max(sfn.col(\"ts\")).alias(\"max_ts\")).first()\n",
    "        return None if row is None else row[0]\n",
    "\n",
    "    def pad_constant_timestamp(self, df: DataFrame, ts) -> DataFrame:\n",
    "        pad_df = df.select(\"tag_name\").distinct().withColumn(\"ts\", ts)\n",
    "        pad_df = pad_df.withColumn(\"value_double\", sfn.lit(None).cast(self.column_types[\"value_double\"]))\n",
    "        return pad_df\n",
    "\n",
    "    @staticmethod\n",
    "    def str2ts(ts_str: str):\n",
    "        return sfn.to_timestamp(sfn.lit(ts_str))\n",
    "\n",
    "    # ---------------------------\n",
    "    # ALL columns + alias\n",
    "    # ---------------------------\n",
    "    def with_alias(self, alias: Union[DataFrame, List[Tuple[str, Optional[str], str, str]]]) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Join filtered historian (df_full) with alias mapping and return:\n",
    "        tag_name, reference_name, controller_name, alias, <all other historian columns>\n",
    "        \"\"\"\n",
    "        if isinstance(alias, list):\n",
    "            alias_df = spark.createDataFrame(\n",
    "                alias, [\"tag_name\", \"reference_name\", \"controller_name\", \"alias\"]\n",
    "            ).dropDuplicates()\n",
    "        else:\n",
    "            alias_df = alias.select(\"tag_name\", \"reference_name\", \"controller_name\", \"alias\").dropDuplicates()\n",
    "\n",
    "        t = self.df_full.alias(\"t\")\n",
    "        m = alias_df.alias(\"m\")\n",
    "\n",
    "        joined = t.join(\n",
    "            m,\n",
    "            (col(\"t.tag_name\") == col(\"m.tag_name\")) &\n",
    "            (col(\"t.controller_name\") == col(\"m.controller_name\")) &\n",
    "            (col(\"t.reference_name\").eqNullSafe(col(\"m.reference_name\"))),\n",
    "            \"inner\"\n",
    "        )\n",
    "\n",
    "        table_cols = [c for c in t.columns if c not in (\"tag_name\", \"reference_name\", \"controller_name\")]\n",
    "        select_exprs = [\n",
    "            col(\"t.tag_name\").alias(\"tag_name\"),\n",
    "            col(\"t.reference_name\").alias(\"reference_name\"),\n",
    "            col(\"t.controller_name\").alias(\"controller_name\"),\n",
    "            col(\"m.alias\").alias(\"alias\"),\n",
    "        ] + [col(f\"t.{c}\") for c in table_cols]\n",
    "\n",
    "        return joined.select(*select_exprs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e63c855-d7a7-48c0-8529-9dc8733cb6f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1.2: Time Format and Widgets for Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e3d312-a455-4fa4-8dbb-17b25dbc2c21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "* **Define dates:** Sets `start_date` to XX (self-define) days before today and `end_date` to today, formatted as `YYYY-MM-DD`.\n",
    "* **Create Databricks widgets:** Exposes parameters you can adjust when running a notebook:\n",
    "\n",
    "  * `start_date`, `end_date` → time window for queries\n",
    "  * `site_name`, `historian_name` → which plant/historian to use\n",
    "  * `site_id`, `site_server` → identifiers for the site and its Proficy server\n",
    "  * `pl_ids`, `pu_ids_base` → IDs for production line and unit\n",
    "  * `debug_mode` → dropdown to toggle debug logging (`yes`/`no`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e430446f-ee6c-4d9d-be1d-53d22d1d9d5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inputs hidden for protecting industrial info\n",
    "time_format = '%Y-%m-%d'\n",
    "time_delta_days = 50\n",
    "end_date = datetime.today()\n",
    "start_date = end_date - timedelta(days=time_delta_days)\n",
    "\n",
    "dbutils.widgets.text(\"start_date\", start_date.strftime(time_format))\n",
    "dbutils.widgets.text(\"end_date\", end_date.strftime(time_format))\n",
    "dbutils.widgets.text(\"site_name\", \"\")\n",
    "dbutils.widgets.text(\"historian_name\", \"\")\n",
    "dbutils.widgets.text(\"site_id\", \"\", \"Site iODS ID\")\n",
    "dbutils.widgets.text(\"site_server\", \"\", \"Site iODS Server\") \n",
    "dbutils.widgets.text(\"pl_ids\", \"\", \"PLID\") \n",
    "dbutils.widgets.text(\"pu_ids_base\", \"\", \"PUID Base\") \n",
    "dbutils.widgets.dropdown(\"debug_mode\", \"yes\",  [\"yes\", \"no\"], \"Debug Mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f278412-b0ff-4972-b732-c22007883a14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 2: Reading Influx Data and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf9ffa4-aaa5-4140-a8d0-57252b10ebe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define tags with actual PLC tag names, references, and PLC controller. Define the alias for each tag for downstream use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edc1ddfb-9422-41d5-b184-e4bbe3b4eb50",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Anonymized process signal definitions (industrial details hidden)\n",
    "# Aliases are kept unchanged for thesis consistency\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "tag_names = [\n",
    "    \"TAG_SPEED_SETPOINT\",\n",
    "    \"TAG_SPEED_ACTUAL\",\n",
    "\n",
    "    \"TAG_METERING_PITCH_SP\",\n",
    "\n",
    "    \"TAG_UNW_ROLL1_DIAMETER\",\n",
    "    \"TAG_UNW_ROLL2_DIAMETER\",\n",
    "    \"TAG_UNW_ROLL1_ACTIVE\",\n",
    "    \"TAG_UNW_ROLL2_ACTIVE\",\n",
    "    \"TAG_LOADCELL_BLC1\",\n",
    "    \"TAG_LOADCELL_BLC2\",\n",
    "    \"TAG_LOADCELL_UNW\",\n",
    "    \"TAG_ENL_UNW_WIDTH\",\n",
    "    \"TAG_ENL_UNW_MIDDLE\",\n",
    "\n",
    "    \"TAG_ENL_BLC1_WIDTH\",\n",
    "    \"TAG_ENL_BLC1_MIDDLE\",\n",
    "\n",
    "    \"TAG_BLC_OMEGA1_PITCH\",\n",
    "]\n",
    "\n",
    "ref_names = [\n",
    "    \"ref_line_speed_sp\",\n",
    "    \"ref_line_speed_actual\",\n",
    "\n",
    "    \"ref_metering_pitch_sp\",\n",
    "\n",
    "    \"ref_unw1_diameter\",\n",
    "    \"ref_unw2_diameter\",\n",
    "    \"ref_unw1_active\",\n",
    "    \"ref_unw2_active\",\n",
    "    \"ref_blc1_loadcell\",\n",
    "    \"ref_blc2_loadcell\",\n",
    "    \"ref_unw_loadcell\",\n",
    "    \"ref_enl_unw_width\",\n",
    "    \"ref_enl_unw_middle\",\n",
    "\n",
    "    \"ref_enl_blc1_width\",\n",
    "    \"ref_enl_blc1_middle\",\n",
    "\n",
    "    \"ref_blc_omega1_pitch\",\n",
    "]\n",
    "\n",
    "ctrl_names = [\n",
    "    \"CTRL_MAIN\",\n",
    "    \"CTRL_MAIN\",\n",
    "\n",
    "    \"CTRL_METERING\",\n",
    "\n",
    "    \"CTRL_UNW\",\n",
    "    \"CTRL_UNW\",\n",
    "    \"CTRL_UNW\",\n",
    "    \"CTRL_UNW\",\n",
    "    \"CTRL_GEO_A\",\n",
    "    \"CTRL_GEO_A\",\n",
    "    \"CTRL_UNW\",\n",
    "    \"CTRL_UNW\",\n",
    "    \"CTRL_UNW\",\n",
    "\n",
    "    \"CTRL_GEO_B\",\n",
    "    \"CTRL_GEO_B\",\n",
    "\n",
    "    \"CTRL_GEO_B\",\n",
    "]\n",
    "\n",
    "aliases = [\n",
    "    \"Set Speed\",\n",
    "    \"Current Speed\",\n",
    "\n",
    "    \"Metering Omega Pitch\",\n",
    "\n",
    "    \"UNW1 Diameter\",\n",
    "    \"UNW2 Diameter\",\n",
    "    \"UNW1 Active Status\",\n",
    "    \"UNW2 Active Status\",\n",
    "    \"BLC1 Loadcell (Before Omega1)\",\n",
    "    \"BLC2 Loadcell (Before ChillRoll)\",\n",
    "    \"UNW Loadcell (Before Metering Omega)\",\n",
    "    \"EnL UNW BLC Width\",\n",
    "    \"EnL UNW Middle\",\n",
    "\n",
    "    \"EnL BLC1 Width\",\n",
    "    \"EnL BLC1 Middle\",\n",
    "\n",
    "    \"BLC Omega 1 Pitch\",\n",
    "]\n",
    "\n",
    "# Unified mapping used throughout the pipeline\n",
    "alias_map = list(zip(tag_names, ref_names, ctrl_names, aliases))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc22754-c47b-457c-b441-0162e06a42cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2.1: Reading Influx Data for Defined Setpoints (and Resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a389c4-8ea1-4e2b-babe-bf23aa254218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* **Purpose:** Read widget inputs, align the time window to **06:00 → 06:00**, and fetch + regularize historian data.\n",
    "* **Timestamps:** `convert_to_timestamp()` turns `YYYY-MM-DD` into `YYYY-MM-DD 06:00:00` for both start/end.\n",
    "* **Query setup:** Builds `HistorianQuery` in **triplet mode** (parallel `tag_names`, `ref_names`, `ctrl_names`) with `sample_freq=\"1 minute\"` and a **forward-fill timeout** of 15 minutes.\n",
    "* **Rich view:** `with_alias(alias_map)` returns **all historian columns + a human-readable `alias`** for joins/plots.\n",
    "* **Regularized series:** `resample(start_ts, end_ts, alias_map=...)` outputs a 1-min grid with `tag_name, reference_name, controller_name, alias, ts, value_double, orig_ts`.\n",
    "* **Remember to define:** `tag_names`, `ref_names`, `ctrl_names`, and `alias_map` beforehand; ensure the table has the required historian columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13394090-2a13-42a2-8ca7-4d8105e5b976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Database:** cdl_stc_prod\n",
    "- **Schema:** silver_mfg_ot\n",
    "- **Table:** influxhistorian_fem_timeseries\n",
    "Source: https://datacatalog.pg.com/table/1937067/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6839ec23-859d-4607-a3a1-761d95837c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# --- Inputs from widgets ---\n",
    "table_name     = \"cdl_stc_prod.silver_mfg_ot.influxhistorian_fem_timeseries\"\n",
    "site           = dbutils.widgets.get(\"site_name\")\n",
    "historian_name = dbutils.widgets.get(\"historian_name\")\n",
    "start_date     = dbutils.widgets.get(\"start_date\")   # \"YYYY-MM-DD\"\n",
    "end_date       = dbutils.widgets.get(\"end_date\")     # \"YYYY-MM-DD\"\n",
    "start_hour     = 6  # 06:00 window alignment\n",
    "\n",
    "# --- Helper to build \"YYYY-MM-DD HH:MM:SS\" at a fixed hour (e.g., 06:00) ---\n",
    "def convert_to_timestamp(date_str: str, hour: int, minute: int = 0) -> str:\n",
    "    dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    dt = dt.replace(hour=hour, minute=minute, second=0, microsecond=0)\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "start_ts = convert_to_timestamp(start_date, start_hour)  \n",
    "end_ts   = convert_to_timestamp(end_date,   start_hour)  \n",
    "\n",
    "# --- Build HistorianQuery using triplets (tag_names, ref_names, ctrl_names) ---\n",
    "hq = HistorianQuery(\n",
    "    table_name,\n",
    "    tag_names, ref_names, ctrl_names,\n",
    "    sample_freq=\"10 seconds\",\n",
    "    ff_timeout=\"2 minutes\",\n",
    "    site=site,\n",
    "    historian_name=historian_name,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    ignore_nulls=False\n",
    ")\n",
    "\n",
    "# --- Rich table: ALL historian columns + alias (first 4 ordered) ---\n",
    "df_all_plus_alias = hq.with_alias(alias_map)\n",
    "display(df_all_plus_alias)\n",
    "\n",
    "# --- Resampled TS on a 1-min grid, aligned 06:00→06:00, with reference_name + controller_name + alias ---\n",
    "df_resampled = hq.resample(\n",
    "    start_ts,\n",
    "    end_ts, \n",
    "    alias_map=alias_map    # ensures columns: tag_name, reference_name, controller_name, alias, ts, value_double, orig_ts\n",
    ").persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "440fc0de-4558-4471-a3d2-ea0a55f97f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2.2: Adding Splice Points and Splice Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e63a68b-37ca-48c7-929d-14f4e0b8252b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* **Aligns series:** Extracts a unique time index from `df_resampled`, then left-joins two roll status signals (`UNW1/UNW2 Active Status`) onto it.\n",
    "* **Flags active states:** Converts each status to binary (`> 0.5 → 1`), giving `st1_active` and `st2_active`.\n",
    "* **Detects splices:** Finds a **handoff** moment where the previously active roll goes 1→0 while the other is 1 (sets `splice_point = 1`).\n",
    "* **Builds a window:** Marks a **±WINDOW\\_MIN minutes** window around each splice using a seconds-based range window on `ts_long` → `splice_window = 1`.\n",
    "* **Merges back:** Joins `splice_point`/`splice_window` back to the full long table; missing flags default to 0.\n",
    "* **Unit fix for sensors:** For specific EnL width/edge aliases, divides `value_double` by **10** to put them on the correct scale; leaves others unchanged.\n",
    "\n",
    "*Notes:* `WINDOW_MIN = 1` (so ±1 minute). The splice condition assumes exactly one roll active at a time and transitions are clean; adjust if overlaps or gaps occur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2222829-2eff-40c0-812a-ecba6f8b16fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Splice features only (no exclusion) ---\n",
    "\n",
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "# ---------- parameters ----------\n",
    "THRESH = 0.5             # >0.5 means \"active\"\n",
    "PRE_MIN  = 3            # minutes BEFORE splice to mark as \"in splice\"\n",
    "POST_MIN = 5             # minutes AFTER splice to mark as \"in splice\"\n",
    "ALIAS_ST1 = \"UNW1 Active Status\"   # Roll_St1_Active\n",
    "ALIAS_ST2 = \"UNW2 Active Status\"   # Roll_St2_Active\n",
    "\n",
    "# Optional: small debounce in ticks to avoid chatter (3-tick mean > 0.5)\n",
    "DEBOUNCE_TICKS = 3       # if your cadence is ~10s, this ≈30s smoothing; if 1min, it's 3min\n",
    "\n",
    "# ---------- build aligned status frame ----------\n",
    "time_df = df_resampled.select(\"ts\").distinct()\n",
    "\n",
    "st1_df = (df_resampled\n",
    "          .filter(F.col(\"alias\") == ALIAS_ST1)\n",
    "          .select(\"ts\", F.col(\"value_double\").alias(\"st1_val\")))\n",
    "st2_df = (df_resampled\n",
    "          .filter(F.col(\"alias\") == ALIAS_ST2)\n",
    "          .select(\"ts\", F.col(\"value_double\").alias(\"st2_val\")))\n",
    "\n",
    "aligned = (time_df.join(st1_df, \"ts\", \"left\")\n",
    "                  .join(st2_df, \"ts\", \"left\")\n",
    "                  .orderBy(\"ts\"))\n",
    "\n",
    "# ---------- binary active flags with debounce ----------\n",
    "w_debounce = Window.orderBy(\"ts\").rowsBetween(-(DEBOUNCE_TICKS-1), 0)\n",
    "\n",
    "aligned = (aligned\n",
    "    .withColumn(\"st1_raw\", (F.col(\"st1_val\") > THRESH).cast(\"int\"))\n",
    "    .withColumn(\"st2_raw\", (F.col(\"st2_val\") > THRESH).cast(\"int\"))\n",
    "    .withColumn(\"st1_active\", (F.avg(\"st1_raw\").over(w_debounce) > 0.5).cast(\"int\"))\n",
    "    .withColumn(\"st2_active\", (F.avg(\"st2_raw\").over(w_debounce) > 0.5).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# ---------- detect splice point (hand-over moment) ----------\n",
    "w_ts = Window.orderBy(\"ts\")\n",
    "aligned = (aligned\n",
    "    .withColumn(\"st1_prev\", F.lag(\"st1_active\").over(w_ts))\n",
    "    .withColumn(\"st2_prev\", F.lag(\"st2_active\").over(w_ts))\n",
    "    .withColumn(\n",
    "        \"splice_point\",\n",
    "        F.when(\n",
    "            ((F.col(\"st1_prev\") == 1) & (F.col(\"st1_active\") == 0) & (F.col(\"st2_active\") == 1)) |\n",
    "            ((F.col(\"st2_prev\") == 1) & (F.col(\"st2_active\") == 0) & (F.col(\"st1_active\") == 1)),\n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------- splice window flag (asymmetric: [-PRE, +POST] minutes) ----------\n",
    "aligned = aligned.withColumn(\"ts_long\", F.col(\"ts\").cast(\"long\"))  # epoch seconds\n",
    "w_range = Window.orderBy(\"ts_long\").rangeBetween(-PRE_MIN*60, POST_MIN*60)\n",
    "aligned = aligned.withColumn(\"splice_flag\", (F.max(\"splice_point\").over(w_range) > 0).cast(\"int\"))\n",
    "\n",
    "# ---------- current active roll (1 or 2) ----------\n",
    "aligned = aligned.withColumn(\n",
    "    \"current_roll\",\n",
    "    F.when((F.col(\"st1_active\")==1) & (F.col(\"st2_active\")==0), F.lit(1))\n",
    "     .when((F.col(\"st2_active\")==1) & (F.col(\"st1_active\")==0), F.lit(2))\n",
    "     .otherwise(F.lit(None).cast(\"int\"))  # ambiguous/transition\n",
    ")\n",
    "\n",
    "# ---------- time since last splice (minutes, continuous feature) ----------\n",
    "w_cum = Window.orderBy(\"ts\")\n",
    "aligned = aligned.withColumn(\"splice_id\", F.sum(\"splice_point\").over(w_cum))\n",
    "\n",
    "# last splice timestamp up to t\n",
    "w_seg = Window.partitionBy(\"splice_id\").orderBy(\"ts\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "aligned = aligned.withColumn(\n",
    "    \"last_splice_ts\",\n",
    "    F.max(F.when(F.col(\"splice_point\")==1, F.col(\"ts\"))).over(w_seg)\n",
    ")\n",
    "\n",
    "aligned = aligned.withColumn(\n",
    "    \"time_since_splice_min\",\n",
    "    F.when(F.col(\"last_splice_ts\").isNotNull(),\n",
    "           (F.col(\"ts\").cast(\"long\") - F.col(\"last_splice_ts\").cast(\"long\"))/60.0)\n",
    "     .otherwise(None)\n",
    ")\n",
    "\n",
    "# ---------- select splice features ----------\n",
    "splice_feats = aligned.select(\n",
    "    \"ts\",\n",
    "    \"splice_point\",          # 1 only at the hand-over tick\n",
    "    \"splice_flag\",           # 0/1 feature: within splice window\n",
    "    \"current_roll\",          # 1 or 2 (or null at transition)\n",
    "    \"time_since_splice_min\"  # continuous stabilization feature\n",
    ")\n",
    "\n",
    "# ---------- join back to the long dataframe (no exclusion) ----------\n",
    "df_with_splice_unscaled = (df_resampled\n",
    "    .join(splice_feats, \"ts\", \"left\")\n",
    "    .fillna({\"splice_point\": 0, \"splice_flag\": 0})\n",
    ")\n",
    "\n",
    "# ---------- EnL scaling (keep your behavior) ----------\n",
    "enl_aliases = [\n",
    "    \"EnL UNW BLC Width\",\n",
    "    \"EnL UNW Right\",\n",
    "    \"EnL UNW Left\",\n",
    "    \"EnL BLC1 Right\",\n",
    "    \"EnL BLC1 Left\"\n",
    "]\n",
    "\n",
    "df_with_splice = df_with_splice_unscaled.withColumn(\n",
    "    \"value_double\",\n",
    "    F.when(F.col(\"alias\").isin(enl_aliases), F.col(\"value_double\") / 10.0)\n",
    "     .otherwise(F.col(\"value_double\"))\n",
    ")\n",
    "\n",
    "display(df_with_splice.orderBy(F.desc(\"ts\")).limit(100))\n",
    "# df_with_splice now has splice features ready to be pivoted/widened and used alongside tensions/modulus/widths/etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ddfdf2b-bfb4-45cb-acd8-b93e4cc0c88c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2.3: Computing E-Modulus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4fc967b-1326-44b7-b85b-ecac89bd199c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "* **Bring flags:** Pulls unique `splice_point`/`splice_window` per timestamp once.\n",
    "* **Pivot wide:** Turns the long table into one row per `ts` with columns for each `alias` so formulas can reference signals directly.\n",
    "* **Prepare inputs:** Maps needed signals (A, B, E, I, G) and converts width from mm→m.\n",
    "* **Safe formula:** Computes `e_modulus` with guards—if any input is null or a denominator becomes 0, returns `NULL` to avoid bad values.\n",
    "* **Reattach labels:** Joins splice flags back onto the calculated result by `ts`.\n",
    "* **Shape to long:** Builds new rows matching the **exact schema** of the original long dataframe (`df_with_splice`), filling non-relevant columns with typed nulls and setting `alias=\"e_modulus\"`.\n",
    "* **Append:** Unions the new `e_modulus` rows with the original data → final `df_with_emodulus` includes both raw tags and the computed metric, ready for downstream analysis/plots.\n",
    "\n",
    "$$\n",
    "E \\;=\\; \\frac{B}{E_m \\cdot \\left(\\frac{0.001 \\cdot I \\cdot (A - B)}{\\big(0.001 \\cdot I \\cdot A - 0.001 \\cdot G \\cdot B\\big)} - 1\\right)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* A = **UNW Loadcell (Before Metering Omega)** \\[N]\n",
    "* B = **BLC1 Loadcell (Before Omega1)** \\[N]\n",
    "* E_m = **EnL UNW BLC Width** \\[m] (converted from mm ÷ 1000)\n",
    "* I = **BLC Omega1 Pitch** \\[mm/pad]\n",
    "* G = **Metering Omega Pitch** \\[mm/pad]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ac2456-33f1-48ce-9408-a2f1e88c5034",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ts\":254,\"orig_ts\":272},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764698818026}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --- 0) Bring splice flags (once) ---\n",
    "flags = (\n",
    "    df_with_splice.select(\"ts\", \"splice_point\", \"splice_flag\").distinct()\n",
    ")\n",
    "\n",
    "# --- 1) Pivot wide so we can reference aliases in one row per ts ---\n",
    "# We pivot only the two columns we need to compute: ts, alias/value\n",
    "wide = (\n",
    "    df_with_splice\n",
    "    .select(\"ts\", \"alias\", \"value_double\")\n",
    "    .groupBy(\"ts\")\n",
    "    .pivot(\"alias\")\n",
    "    .agg(F.first(\"value_double\"))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Required inputs (using your symbols):\n",
    "# A = UNW Loadcell (N)\n",
    "# B = BLC1 Loadcell (Before Omega1) (N)\n",
    "# E = EnL UNW BLC Width (mm)   -> meters in the formula\n",
    "# I = BLC1/BLC Omega1 Pitch (mm/pads)\n",
    "# G = Metering Omega Pitch (mm/pads)\n",
    "\n",
    "A = F.col(\"UNW Loadcell (Before Metering Omega)\")\n",
    "B = F.col(\"BLC1 Loadcell (Before Omega1)\")\n",
    "E_m = (F.col(\"EnL UNW BLC Width\") / F.lit(1000.0))  # mm -> m\n",
    "I = F.col(\"BLC Omega 1 Pitch\")\n",
    "G = F.col(\"Metering Omega Pitch\")\n",
    "\n",
    "# Inner pieces of the formula\n",
    "denom_inner = (A * 0.001 * I) - (B * 0.001 * G)\n",
    "ratio_term = (0.001 * I * (A - B)) / denom_inner\n",
    "\n",
    "# Guard conditions: if any needed term is null OR denominators == 0 => NULL\n",
    "safe_emod = F.when(\n",
    "    (A.isNull()) | (B.isNull()) | (E_m.isNull()) | (I.isNull()) | (G.isNull()) |\n",
    "    (denom_inner == 0) | ((E_m * (ratio_term - 1)) == 0),\n",
    "    F.lit(None).cast(\"double\")\n",
    ").otherwise(\n",
    "    B / (E_m * (ratio_term - 1))\n",
    ")\n",
    "\n",
    "# Compute e_modulus per timestamp\n",
    "e_calc = (\n",
    "    wide\n",
    "    .select(\n",
    "        F.col(\"ts\"),\n",
    "        safe_emod.alias(\"e_modulus\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Attach splice flags so the new rows carry the same labels at that ts\n",
    "e_calc = e_calc.join(flags, \"ts\", \"left\")\n",
    "\n",
    "# --- 3) Build rows with the FULL schema of df_with_splice ---\n",
    "# We’ll create the new “e_modulus” rows with the same columns (in order)\n",
    "schema = df_with_splice.schema\n",
    "cols = [f.name for f in schema]\n",
    "types = {f.name: f.dataType for f in schema}\n",
    "\n",
    "def typed_null(colname):\n",
    "    return F.lit(None).cast(types[colname]).alias(colname)\n",
    "\n",
    "select_exprs = []\n",
    "for c in cols:\n",
    "    if c == \"tag_name\":\n",
    "        select_exprs.append(F.lit(\"e_modulus_tag\").alias(c))\n",
    "    elif c == \"reference_name\":\n",
    "        select_exprs.append(F.lit(\"e_modulus_cal\").alias(c))\n",
    "    elif c == \"alias\":\n",
    "        select_exprs.append(F.lit(\"e_modulus\").alias(c))\n",
    "    elif c == \"ts\":\n",
    "        select_exprs.append(F.col(\"ts\").alias(c))\n",
    "    elif c == \"value_double\":\n",
    "        select_exprs.append(F.col(\"e_modulus\").alias(c))\n",
    "    elif c == \"splice_point\":\n",
    "        select_exprs.append(F.col(\"splice_point\").cast(types[c]).alias(c))\n",
    "    elif c == \"splice_flag\":\n",
    "        select_exprs.append(F.col(\"splice_flag\").cast(types[c]).alias(c))\n",
    "    else:\n",
    "        select_exprs.append(typed_null(c))\n",
    "\n",
    "e_rows_long = e_calc.select(*select_exprs)\n",
    "\n",
    "# --- 4) Append (union) these rows to the original long-format DF ---\n",
    "df_with_emodulus = df_with_splice.unionByName(e_rows_long)\n",
    "\n",
    "\n",
    "display(df_with_emodulus.orderBy(F.desc(\"ts\")).limit(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e6ddab-118b-4916-b5ad-4afd2bf8fc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2.4: Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "077afa55-fa79-4892-b06d-9f616509588a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ts\":294},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764699693026}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window as W  # safer import\n",
    "\n",
    "# --- carry splice/context features (ONE ROW PER ts) ---\n",
    "flags = (\n",
    "    df_with_emodulus\n",
    "      .groupBy(\"ts\")\n",
    "      .agg(\n",
    "          F.max(\"splice_flag\").alias(\"splice_flag\"),\n",
    "          F.first(\"current_roll\", ignorenulls=True).alias(\"current_roll\"),\n",
    "          F.first(\"time_since_splice_min\", ignorenulls=True).alias(\"time_since_splice_min\"),\n",
    "      )\n",
    ")\n",
    "\n",
    "# --- pivot to wide (one row per ts, one column per alias) ---\n",
    "wide = (\n",
    "    df_with_emodulus\n",
    "      .select(\"ts\", \"alias\", \"value_double\")\n",
    "      .groupBy(\"ts\")\n",
    "      .pivot(\"alias\")\n",
    "      .agg(F.first(\"value_double\"))\n",
    "      .orderBy(\"ts\")\n",
    ")\n",
    "\n",
    "# join splice flags (no dupes now)\n",
    "wide = wide.join(flags, \"ts\", \"left\").fillna({\"splice_flag\": 0})\n",
    "\n",
    "# optional one-hots for roll\n",
    "wide = (\n",
    "    wide\n",
    "    .withColumn(\"roll1_flag\", F.when(F.col(\"current_roll\") == 1, 1).otherwise(0))\n",
    "    .withColumn(\"roll2_flag\", F.when(F.col(\"current_roll\") == 2, 1).otherwise(0))\n",
    ")\n",
    "\n",
    "# =====================================================================\n",
    "# 1) Define which signals get which type of rolling features\n",
    "# =====================================================================\n",
    "\n",
    "full_rolling_signals = [\n",
    "    \"BLC1 Loadcell (Before Omega1)\",\n",
    "    \"BLC2 Loadcell (Before ChillRoll)\",\n",
    "    \"e_modulus\",\n",
    "]\n",
    "\n",
    "mean_std_only_signals = [\n",
    "    \"UNW Loadcell (Before Metering Omega)\",\n",
    "    \"EnL UNW BLC Width\",\n",
    "    \"EnL UNW Middle\",\n",
    "]\n",
    "\n",
    "base_signals_only = [\n",
    "    \"UNW1 Diameter\",\n",
    "    \"UNW2 Diameter\",\n",
    "]\n",
    "\n",
    "speed_signals = [\"Current Speed\", \"Set Speed\", \"Linear Line Speed\"]\n",
    "\n",
    "# Time window definitions (range in *seconds* based on ts_long)\n",
    "wide = wide.withColumn(\"ts_long\", F.col(\"ts\").cast(\"long\"))\n",
    "\n",
    "def time_win(mins: int):\n",
    "    return W.orderBy(\"ts_long\").rangeBetween(-int(mins * 60), 0)\n",
    "\n",
    "WINS_FULL = [3, 5, 10]   # minutes\n",
    "WINS_LIGHT = [5, 10]     # minutes\n",
    "\n",
    "# =====================================================================\n",
    "# 2) Helpers for adding rolling features\n",
    "# =====================================================================\n",
    "\n",
    "def add_full_window_feats(df, colname, wmin):\n",
    "    \"\"\"pctchg + slope + mean + std for core signals.\"\"\"\n",
    "    if colname not in df.columns:\n",
    "        return df\n",
    "\n",
    "    w = time_win(wmin)\n",
    "    x = F.col(colname)\n",
    "\n",
    "    med   = F.percentile_approx(x, 0.5, 10000).over(w)\n",
    "    mean  = F.avg(x).over(w)\n",
    "    std   = F.stddev_samp(x).over(w)\n",
    "    first = F.first(x, ignorenulls=True).over(w)\n",
    "\n",
    "    pctchg = (x - med) / F.when(F.abs(med) > 0, F.abs(med))\n",
    "    slope  = (x - first) / F.lit(float(wmin))\n",
    "\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(f\"{colname}__pctchg_med_{wmin}m\", pctchg)\n",
    "        .withColumn(f\"{colname}__slope_{wmin}m\",      slope)\n",
    "        .withColumn(f\"{colname}__mean_{wmin}m\",       mean)\n",
    "        .withColumn(f\"{colname}__std_{wmin}m\",        std)\n",
    "    )\n",
    "\n",
    "def add_mean_std_feats(df, colname, wmin):\n",
    "    \"\"\"Only mean + std (no pctchg, no slope).\"\"\"\n",
    "    if colname not in df.columns:\n",
    "        return df\n",
    "\n",
    "    w = time_win(wmin)\n",
    "    x = F.col(colname)\n",
    "\n",
    "    mean = F.avg(x).over(w)\n",
    "    std  = F.stddev_samp(x).over(w)\n",
    "\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(f\"{colname}__mean_{wmin}m\", mean)\n",
    "        .withColumn(f\"{colname}__std_{wmin}m\",  std)\n",
    "    )\n",
    "\n",
    "# =====================================================================\n",
    "# 3) Apply rolling features, but only where we actually need them\n",
    "# =====================================================================\n",
    "\n",
    "dfF = wide\n",
    "\n",
    "for s in full_rolling_signals:\n",
    "    for wmin in WINS_FULL:\n",
    "        dfF = add_full_window_feats(dfF, s, wmin)\n",
    "\n",
    "for s in mean_std_only_signals:\n",
    "    for wmin in WINS_LIGHT:\n",
    "        dfF = add_mean_std_feats(dfF, s, wmin)\n",
    "\n",
    "# joint/group effects (still optional)\n",
    "t1 = \"BLC1 Loadcell (Before Omega1)__pctchg_med_5m\"\n",
    "t2 = \"BLC2 Loadcell (Before ChillRoll)__pctchg_med_5m\"\n",
    "em = \"e_modulus__pctchg_med_5m\"\n",
    "\n",
    "if all(c in dfF.columns for c in (t1, t2, em)):\n",
    "    dfF = (\n",
    "        dfF\n",
    "        .withColumn(\n",
    "            \"joint_energy_t1_t2_em_5m\",\n",
    "            F.sqrt(F.col(t1) * F.col(t1) + F.col(t2) * F.col(t2) + F.col(em) * F.col(em)),\n",
    "        )\n",
    "        .withColumn(\"t1_t2_interact_5m\", F.col(t1) * F.col(t2))\n",
    "        .withColumn(\"t1_em_interact_5m\", F.col(t1) * F.col(em))\n",
    "        .withColumn(\"t2_em_interact_5m\", F.col(t2) * F.col(em))\n",
    "    )\n",
    "\n",
    "# =====================================================================\n",
    "# 4) Final feature selection into fv_blc\n",
    "# =====================================================================\n",
    "\n",
    "present_speed       = [c for c in speed_signals         if c in dfF.columns]\n",
    "present_full_base   = [c for c in full_rolling_signals  if c in dfF.columns]\n",
    "present_light_base  = [c for c in mean_std_only_signals if c in dfF.columns]\n",
    "present_base_levels = [c for c in base_signals_only     if c in dfF.columns]\n",
    "\n",
    "core_cols = [\n",
    "    \"ts\",\n",
    "    \"splice_flag\",\n",
    "    \"current_roll\",\n",
    "    \"time_since_splice_min\",\n",
    "    \"roll1_flag\",\n",
    "    \"roll2_flag\",\n",
    "]\n",
    "core_cols = [c for c in core_cols if c in dfF.columns]\n",
    "\n",
    "generated_cols = [c for c in dfF.columns if \"__\" in c]\n",
    "\n",
    "keep_cols = (\n",
    "    core_cols\n",
    "    + present_speed\n",
    "    + present_full_base\n",
    "    + present_light_base\n",
    "    + present_base_levels\n",
    "    + generated_cols\n",
    ")\n",
    "\n",
    "keep_cols = [c for c in keep_cols if c in dfF.columns]\n",
    "\n",
    "fv_blc = dfF.select(*keep_cols)\n",
    "\n",
    "# --- sanity check: should be 1 row per ts ---\n",
    "n_rows = fv_blc.count()\n",
    "n_ts   = fv_blc.select(\"ts\").distinct().count()\n",
    "print(\"rows in fv_blc:        \", n_rows)\n",
    "print(\"distinct ts in fv_blc: \", n_ts)\n",
    "\n",
    "display(fv_blc.orderBy(F.desc(\"ts\")).limit(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f25013d-d1f5-4ca7-9a00-b3473e9c570d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2.5: Computing Linear Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e05c24d-86d1-4d13-9827-3ead9944783d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "* Pulls **Current Speed** and **Metering Omega Pitch** per timestamp, outer-joins them.\n",
    "* Computes **Linear Line Speed = speed × pitch** (else **0** if missing or ≤0).\n",
    "* Stamps fixed metadata (alias/tag/reference/controller, flags, timers, `orig_ts`).\n",
    "* Reorders columns to match `df_with_emodulus` and **unions** the new rows in.\n",
    "\n",
    "**Notes to sanity-check:**\n",
    "\n",
    "* Ensure units: if `speed` is **pads/min** and `pitch` is **mm/pad**, result is **mm/min** (✓).\n",
    "  If `speed` is **m/min**, multiply by **pads/m** or convert `pitch` to meters first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee03453-ed97-4b5f-953b-69d3481b47a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Add Linear Line Speed directly in WIDE ---\n",
    "SPEED_COL = \"Current Speed\"          # update if your alias differs\n",
    "PITCH_COL = \"Metering Omega Pitch\"   # update if your alias differs\n",
    "LINEAR_COL = \"Linear Line Speed\"\n",
    "\n",
    "if (SPEED_COL in wide.columns) and (PITCH_COL in wide.columns):\n",
    "    wide = wide.withColumn(\n",
    "        LINEAR_COL,\n",
    "        F.when((F.col(SPEED_COL) > 0) & (F.col(PITCH_COL) > 0),\n",
    "               (F.col(SPEED_COL) * F.col(PITCH_COL)).cast(\"double\"))\n",
    "         .otherwise(F.lit(0.0))\n",
    "    )\n",
    "else:\n",
    "    # If one of the inputs is missing, we just skip creating the feature (no breakage)\n",
    "    pass\n",
    "\n",
    "signals = [\n",
    "    \"BLC1 Loadcell (Before Omega1)\",\n",
    "    \"BLC2 Loadcell (Before ChillRoll)\",\n",
    "    \"UNW Loadcell (Before Metering Omega)\",\n",
    "    \"EnL UNW BLC Width\",\n",
    "    \"EnL UNW Middle\",\n",
    "    \"UNW1 Diameter\",\n",
    "    \"UNW2 Diameter\",\n",
    "    \"e_modulus\",\n",
    "    \"Linear Line Speed\"  # <-- newly added\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd30c8ba-f884-474b-9db4-c01eca10900d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# after you've added Linear Line Speed into `wide` as you showed\n",
    "speedish = [\"Current Speed\", \"Set Speed\", \"Metering Omega Pitch\", \"Linear Line Speed\"]\n",
    "present_speedish = [c for c in speedish if c in dfF.columns]\n",
    "\n",
    "keep_cols = (\n",
    "    [\"ts\", \"splice_flag\", \"current_roll\", \"time_since_splice_min\", \"roll1_flag\", \"roll2_flag\"]\n",
    "    + present_speedish\n",
    "    + [c for c in dfF.columns if c in signals or \"__\" in c]  # all engineered features + base signals\n",
    ")\n",
    "\n",
    "fv_blc = dfF.select(*keep_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53dff884-9c73-4d49-b2ea-66128175f94d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 3: Proficy Historian / IODS Downtime Data (PySpark SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26617235-bb9d-439f-973a-46c4c4c87650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Data Source: Proficy Historian**\n",
    "- Database in CDL (table name and column names anonymized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32e5a468-9ee1-445e-9e97-9de1c904efad",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764920471221}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Anonymized downtime / uptime extraction query\n",
    "# (industrial identifiers and reason texts removed)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    t.line_description,\n",
    "    t.start_time,\n",
    "    t.end_time,\n",
    "    t.start_time_utc,\n",
    "    t.end_time_utc,\n",
    "    t.reason_lvl1,\n",
    "    t.reason_lvl2,\n",
    "    t.reason_lvl3,\n",
    "    t.reason_lvl4,\n",
    "    t.operator_comment,\n",
    "    t.team_description,\n",
    "    t.product_code,\n",
    "    t.product_description,\n",
    "    t.event_duration,\n",
    "    t.uptime,\n",
    "    t.fault_flag,\n",
    "    t.fault_code,\n",
    "    t.planned_flag,\n",
    "    t.location_code,\n",
    "    t.process_unit,\n",
    "    t.manual_stop_flag,\n",
    "    t.minor_stop_flag,\n",
    "    t.major_stop_flag,\n",
    "    t.is_blocked,\n",
    "    t.is_constraint,\n",
    "    t.is_starved\n",
    "FROM analytics_layer.production_events.downtime_events AS t\n",
    "WHERE event_date BETWEEN '{dbutils.widgets.get(\"start_date\")}'\n",
    "                      AND '{dbutils.widgets.get(\"end_date\")}'\n",
    "  AND business_unit = 'business_unit_X'\n",
    "  AND site_identifier = '{dbutils.widgets.get(\"site_id\")}'\n",
    "  AND delete_flag = FALSE\n",
    "  AND production_line_id = '{dbutils.widgets.get(\"pl_ids\")}'\n",
    "  AND production_line_desc = 'LINE_X'\n",
    "  AND (\n",
    "        reason_lvl1 IN (\n",
    "          'PROCESS_MODULE_A',\n",
    "          'PROCESS_MODULE_B',\n",
    "          'PROCESS_MODULE_C',\n",
    "          'PROCESS_MODULE_D'\n",
    "        )\n",
    "        OR reason_lvl3 IN ('PROCESS_RESOURCE_EXHAUSTED')\n",
    "      )\n",
    "  AND reason_lvl2 NOT IN (\n",
    "        'UPSTREAM_FEED_ISSUE_A',\n",
    "        'UPSTREAM_FEED_ISSUE_B',\n",
    "        'UPSTREAM_FEED_ISSUE_C',\n",
    "        'UPSTREAM_FEED_ISSUE_D'\n",
    "      )\n",
    "\"\"\"\n",
    "\n",
    "df_stops_blc = (\n",
    "    spark.sql(query)\n",
    "         .toPandas()\n",
    "         .sort_values(by=\"start_time_utc\")\n",
    ")\n",
    "\n",
    "display(df_stops_blc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7854d10-54b5-469f-900f-2849099e4097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 4: Data Merge (fv_blc_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f9af6a-e2cf-4300-b93a-e2fef11bbd23",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ts\":216},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764747537325}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "# ---------- 0) Coerce inputs to Spark DFs ----------\n",
    "def to_spark(df):\n",
    "    # already Spark?\n",
    "    if hasattr(df, \"withColumn\"): \n",
    "        return df\n",
    "    # pandas / Koalas -> Spark\n",
    "    import pandas as pd\n",
    "    if isinstance(df, pd.DataFrame) or hasattr(df, \"to_pandas\"):\n",
    "        pdf = df if isinstance(df, pd.DataFrame) else df.to_pandas()\n",
    "        return spark.createDataFrame(pdf)\n",
    "    raise TypeError(\"Provide pandas/Spark/pandas-on-Spark DataFrame.\")\n",
    "\n",
    "fv_blc_sp      = to_spark(fv_blc)\n",
    "df_stops_blc_sp = to_spark(df_stops_blc)\n",
    "\n",
    "# ---------- 1) Ensure expected stop column names exist ----------\n",
    "cols = {c.lower(): c for c in df_stops_blc_sp.columns}\n",
    "start_col = cols.get(\"start_time_utc\", cols.get(\"start_ts\"))\n",
    "end_col   = cols.get(\"end_time_utc\",   cols.get(\"end_ts\"))\n",
    "if start_col is None or end_col is None:\n",
    "    raise KeyError(\"df_stops_blc must have start_time_utc/end_time_utc (or start_ts/end_ts)\")\n",
    "\n",
    "st = (\n",
    "    df_stops_blc_sp\n",
    "      .withColumn(\"start_ts\", F.to_timestamp(F.col(start_col)))\n",
    "      .withColumn(\"end_ts\",   F.to_timestamp(F.col(end_col)))\n",
    "      .select(\"start_ts\",\"end_ts\")\n",
    "      .dropna(subset=[\"start_ts\"])\n",
    ")\n",
    "\n",
    "x = fv_blc_sp.withColumn(\"ts\", F.to_timestamp(\"ts\"))\n",
    "\n",
    "# ---------- 2) Feature 1: is_ts_in_blc_stop ----------\n",
    "inside = (\n",
    "    x.select(\"ts\")\n",
    "     .join(st, (F.col(\"ts\") >= F.col(\"start_ts\")) & (F.col(\"ts\") <= F.col(\"end_ts\")), \"left\")\n",
    "     .withColumn(\"is_ts_in_blc_stop\", F.when(F.col(\"start_ts\").isNotNull(), 1).otherwise(0))\n",
    "     .groupBy(\"ts\").agg(F.max(\"is_ts_in_blc_stop\").alias(\"is_ts_in_blc_stop\"))\n",
    ")\n",
    "\n",
    "# ---------- 3) Feature 2: time_since_failure ----------\n",
    "last_end = (\n",
    "    x.select(\"ts\")\n",
    "     .join(st, F.col(\"end_ts\") <= F.col(\"ts\"), \"left\")\n",
    "     .groupBy(\"ts\").agg(F.max(\"end_ts\").alias(\"last_end_ts\"))\n",
    ")\n",
    "time_since_failure = last_end.withColumn(\n",
    "    \"time_since_failure\",\n",
    "    F.when(F.col(\"last_end_ts\").isNull(), F.lit(float(\"inf\")))\n",
    "     .otherwise((F.col(\"ts\").cast(\"long\") - F.col(\"last_end_ts\").cast(\"long\"))/60.0)\n",
    ").drop(\"last_end_ts\")\n",
    "\n",
    "# ---------- 4) Feature 4: time_to_failure ----------\n",
    "next_start = (\n",
    "    x.select(\"ts\")\n",
    "     .join(st, F.col(\"start_ts\") >= F.col(\"ts\"), \"left\")\n",
    "     .groupBy(\"ts\").agg(F.min(\"start_ts\").alias(\"next_start_ts\"))\n",
    ")\n",
    "time_to_failure = next_start.withColumn(\n",
    "    \"time_to_failure\",\n",
    "    F.when(F.col(\"next_start_ts\").isNull(), F.lit(float(\"inf\")))\n",
    "     .otherwise((F.col(\"next_start_ts\").cast(\"long\") - F.col(\"ts\").cast(\"long\"))/60.0)\n",
    ").drop(\"next_start_ts\")\n",
    "\n",
    "# ---------- 5) Join features back ----------\n",
    "fv_blc_labeled = (\n",
    "    x\n",
    "    .join(inside,           \"ts\", \"left\")\n",
    "    .join(time_since_failure,\"ts\", \"left\")\n",
    "    .join(time_to_failure,  \"ts\", \"left\")\n",
    "    .fillna({\"is_ts_in_blc_stop\": 0})\n",
    ")\n",
    "\n",
    "# ---------- 6) Display ----------\n",
    "display(fv_blc_labeled.orderBy(F.desc(\"ts\")).limit(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c893141-df53-474e-bddf-c3665c862552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4.1 fv_blc_labeled analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c026164d-5c1a-4ab1-bcaa-96a83280a9c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def summarize_fv_blc(df, ts_col=\"ts\"):\n",
    "    # Ensure timestamp column is proper timestamp type\n",
    "    df = df.withColumn(ts_col, F.to_timestamp(ts_col))\n",
    "\n",
    "    print(\"=== 1) Columns and Data Types ===\")\n",
    "    for f in df.schema.fields:\n",
    "        print(f\" - {f.name}: {f.dataType.simpleString()}\")\n",
    "\n",
    "    # --- 2) One-line definition of each feature (auto + custom for labels) ---\n",
    "    descriptions = {}\n",
    "    for c in df.columns:\n",
    "        if c == ts_col:\n",
    "            descriptions[c] = \"Timestamp of the aggregated sample for this row.\"\n",
    "        elif c == \"is_ts_in_blc_stop\":\n",
    "            descriptions[c] = \"Binary label: 1 if ts lies inside a BLC stop interval, else 0.\"\n",
    "        elif c == \"time_since_failure\":\n",
    "            descriptions[c] = \"Minutes elapsed since the end of the most recent BLC stop (∞ if none so far).\"\n",
    "        elif c == \"time_to_failure\":\n",
    "            descriptions[c] = \"Minutes until the next BLC stop starts (∞ if no later stop).\"\n",
    "        else:\n",
    "            # Generic placeholder – you can overwrite these if you want more precise text\n",
    "            descriptions[c] = f\"Feature '{c}' (process variable or engineered feature).\"\n",
    "\n",
    "    print(\"\\n=== 2) Column Descriptions ===\")\n",
    "    for c in df.columns:\n",
    "        print(f\" - {c}: {descriptions[c]}\")\n",
    "\n",
    "    # --- 3) What are labels / what are not ---\n",
    "    label_cols = [\"is_ts_in_blc_stop\", \"time_since_failure\", \"time_to_failure\"]\n",
    "    label_cols_present = [c for c in label_cols if c in df.columns]\n",
    "    feature_cols = [c for c in df.columns if c not in label_cols_present]\n",
    "\n",
    "    print(\"\\n=== 3) Label vs Non-label Columns ===\")\n",
    "    print(\"Label columns (targets):\")\n",
    "    print(\" \", label_cols_present)\n",
    "    print(\"Non-label feature columns:\")\n",
    "    print(\" \", feature_cols)\n",
    "\n",
    "    # --- 4) Total rows ---\n",
    "    n_rows = df.count()\n",
    "    print(\"\\n=== 4) Row Count ===\")\n",
    "    print(f\"Total rows: {n_rows}\")\n",
    "\n",
    "    # --- 5) Time start and time end ---\n",
    "    ts_bounds = df.agg(\n",
    "        F.min(ts_col).alias(\"t_min\"),\n",
    "        F.max(ts_col).alias(\"t_max\")\n",
    "    ).first()\n",
    "\n",
    "    print(\"\\n=== 5) Time Range ===\")\n",
    "    print(f\"Time start: {ts_bounds.t_min}\")\n",
    "    print(f\"Time end  : {ts_bounds.t_max}\")\n",
    "\n",
    "        # --- 6) Frequency of data (timestamp spacing) ---\n",
    "    w = Window.orderBy(ts_col)\n",
    "    df_delta = (\n",
    "        df.select(\n",
    "            F.col(ts_col),\n",
    "            (F.col(ts_col).cast(\"long\") - F.lag(ts_col).over(w).cast(\"long\")).alias(\"delta_s\")\n",
    "        )\n",
    "        .where(F.col(\"delta_s\").isNotNull())\n",
    "    )\n",
    "\n",
    "    # >>> FIX: avoid .rdd.isEmpty() on shared clusters <<<\n",
    "    if df_delta.limit(1).count() == 0:\n",
    "        print(\"\\n=== 6) Frequency ===\")\n",
    "        print(\"Not enough rows to compute timestamp frequency.\")\n",
    "        return\n",
    "\n",
    "    delta_stats = df_delta.agg(\n",
    "        F.min(\"delta_s\").alias(\"min_s\"),\n",
    "        F.expr(\"percentile_approx(delta_s, 0.5)\").alias(\"median_s\"),\n",
    "        F.max(\"delta_s\").alias(\"max_s\"),\n",
    "    ).first()\n",
    "\n",
    "    print(\"\\n=== 6) Timestamp Step / Frequency ===\")\n",
    "    print(f\"Min step   : {delta_stats.min_s} seconds\")\n",
    "    print(f\"Median step: {delta_stats.median_s} seconds\")\n",
    "    print(f\"Max step   : {delta_stats.max_s} seconds\")\n",
    "    if delta_stats.median_s and delta_stats.median_s != 0:\n",
    "        print(f\"Approx. sampling frequency: {1.0 / delta_stats.median_s:.4f} Hz \"\n",
    "              f\"({delta_stats.median_s} s between samples)\")\n",
    "\n",
    "\n",
    "# ---- Call it on your table ----\n",
    "summarize_fv_blc(fv_blc_labeled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78a7d4e-c193-428d-9915-a57a5bb63d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4.2: All rows in is_in_blc_stop = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d4f9b72-bd0c-43b0-a0ae-460c1a0fe835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_stops_only = (\n",
    "    fv_blc_labeled\n",
    "        .filter(F.col(\"is_ts_in_blc_stop\") == 1)\n",
    "        .orderBy(\"ts\")\n",
    ")\n",
    "\n",
    "display(df_stops_only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "101f44a7-8ce3-40e9-a33b-363ef77145c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4.3: Quality Check\n",
    "\n",
    "The code evaluates several logical conditions in the BLC-labeled dataset to verify alignment between process behavior and engineered stop labels. It computes counts for:\n",
    "\n",
    "1. **Rows where the line is fully stopped**\n",
    "   - `Current Speed == 0`\n",
    "   - Validates true downtime moments.\n",
    "\n",
    "2. **Rows labeled as inside a BLC stop**\n",
    "   - `is_ts_in_blc_stop == 1`\n",
    "   - Measures the size of the stop window after joining with the stop table.\n",
    "\n",
    "3. **Rows where actual speed deviates from the set speed**\n",
    "   - `Current Speed != Set Speed`\n",
    "   - Captures acceleration/deceleration and control mismatches.\n",
    "\n",
    "4. **Rows before the first recorded failure**\n",
    "   - `time_since_failure == ∞`\n",
    "   - Indicates initial normal-operation region.\n",
    "\n",
    "5. **Rows after the last recorded failure**\n",
    "   - `time_to_failure == ∞`\n",
    "   - Represents the tail section of the dataset.\n",
    "\n",
    "6. **Rows inside splice intervals**\n",
    "   - `splice_flag == 1`\n",
    "   - Helps distinguish splice-induced disturbances from true BLC failures.\n",
    "\n",
    "7. **Potential label misalignment rows**\n",
    "   - `is_ts_in_blc_stop == 1 AND Current Speed == Set Speed`\n",
    "   - Ideally, during a true failure, current speed should drop quickly.\n",
    "   - These rows highlight timestamps where stop labels may be \"smeared\" due to minute-level stop windows merging with 10-second historian data.\n",
    "\n",
    "These checks validate whether stop labels, splice indicators, and speed dynamics behave consistently, helping ensure high-quality ML training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c18eb08-4b18-4bd0-a807-b2916f245b3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = fv_blc_labeled\n",
    "total_rows = 328320  # fixed total count\n",
    "\n",
    "result = (\n",
    "    df.agg(\n",
    "        # 1) Current Speed == 0\n",
    "        F.sum(F.when(F.col(\"Current Speed\") == 0, 1).otherwise(0)).alias(\"Current Speed = 0\"),\n",
    "\n",
    "        # 2) In BLC stop\n",
    "        F.sum(F.when(F.col(\"is_ts_in_blc_stop\") == 1, 1).otherwise(0)).alias(\"Rows in BLC Stop\"),\n",
    "\n",
    "        # 3) Current Speed != Set Speed AND Current Speed != 0\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                (F.col(\"Current Speed\") != F.col(\"Set Speed\")) &\n",
    "                (F.col(\"Current Speed\") != 0),\n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        ).alias(\"Ramp Up / Ramp Down\"),\n",
    "\n",
    "        # 4) time_since_failure == inf/null\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                F.col(\"time_since_failure\").isNull() |\n",
    "                (F.col(\"time_since_failure\") == float(\"inf\")),\n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        ).alias(\"Infinite Time since Failure\"),\n",
    "\n",
    "        # 5) time_to_failure == inf/null\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                F.col(\"time_to_failure\").isNull() |\n",
    "                (F.col(\"time_to_failure\") == float(\"inf\")),\n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        ).alias(\"Infinite Time to Failure\"),\n",
    "\n",
    "        # 6) splice_flag == 1\n",
    "        F.sum(F.when(F.col(\"splice_flag\") == 1, 1).otherwise(0)).alias(\"Data in Splice Window -3/+5 Mins\"),\n",
    "\n",
    "        # 7) is_ts_in_blc_stop AND Current Speed == Set Speed\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                (F.col(\"is_ts_in_blc_stop\") == 1) &\n",
    "                (F.col(\"Current Speed\") == F.col(\"Set Speed\")),\n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        ).alias(\"Data in Stop (Smerge with Running Line)\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert to Pandas to compute percentage columns\n",
    "pdf = result.toPandas()\n",
    "\n",
    "# Add % columns for each metric\n",
    "for col in pdf.columns:\n",
    "    perc_col = col + \"_perc\"\n",
    "    pdf[perc_col] = (pdf[col] / total_rows) * 100\n",
    "\n",
    "display(pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61f3012a-9462-46c7-abf4-957a83f060c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 5: PCA Setup (All Dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "959cc834-d50e-4a8a-bcf2-1570ed8ffd90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Choose numeric columns & exclude labels / IDs\n",
    "# -----------------------------\n",
    "numeric_cols = [\n",
    "    f.name for f in fv_blc_labeled.schema.fields\n",
    "    if isinstance(f.dataType, NumericType)\n",
    "]\n",
    "\n",
    "cols_to_exclude = [\n",
    "    \"current_roll\",\n",
    "    \"roll1_flag\",\n",
    "    \"roll2_flag\",\n",
    "    \"Metering Omega Pitch\",\n",
    "    \"Set Speed\",\n",
    "    \"is_ts_in_blc_stop\",\n",
    "    \"time_since_failure\",\n",
    "    \"time_to_failure\",\n",
    "]\n",
    "\n",
    "numeric_cols = [c for c in numeric_cols if c not in cols_to_exclude]\n",
    "\n",
    "print(f\"Total numeric columns used for PCA: {len(numeric_cols)}\")\n",
    "print(\"Sample of numeric columns:\", numeric_cols[:15])\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Move data to pandas & scale\n",
    "# -----------------------------\n",
    "# drop rows with any NaN in PCA columns\n",
    "df_pd = (\n",
    "    fv_blc_labeled\n",
    "    .select(numeric_cols)\n",
    "    .dropna(subset=numeric_cols)\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "print(\"Rows going into PCA (Spark -> pandas):\", df_pd.shape[0])\n",
    "print(\"Shape of pandas matrix:\", df_pd.shape)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_pd.values)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Run PCA\n",
    "# -----------------------------\n",
    "k = min(20, X_scaled.shape[1])  # up to 20 PCs or #features\n",
    "pca = PCA(n_components=k, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "eigenvalues = pca.explained_variance_\n",
    "expl_var_ratio = pca.explained_variance_ratio_\n",
    "cum_ratio = np.cumsum(expl_var_ratio)\n",
    "\n",
    "# Table of eigenvalues & explained variance\n",
    "eig_df = pd.DataFrame({\n",
    "    \"PC\": [f\"PC{i+1}\" for i in range(k)],\n",
    "    \"eigenvalue\": eigenvalues,\n",
    "    \"expl_var_ratio\": expl_var_ratio,\n",
    "    \"cum_expl_var_ratio\": cum_ratio,\n",
    "})\n",
    "\n",
    "print(\"\\n=== Eigenvalues & explained variance ===\")\n",
    "display(eig_df)\n",
    "\n",
    "# How many PCs for >=70% variance?\n",
    "num_for_70 = int(np.argmax(cum_ratio >= 0.70) + 1)\n",
    "print(f\"\\nNumber of PCs needed to explain ≥70% variance: {num_for_70}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Scree plot\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, k+1), expl_var_ratio, marker=\"o\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Optional: vertical line at 70% cutoff\n",
    "plt.axvline(num_for_70, linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Feature loadings (which features dominate each PC)\n",
    "# -----------------------------\n",
    "components = pca.components_  # shape (k, n_features)\n",
    "loadings = pd.DataFrame(\n",
    "    components.T,\n",
    "    index=numeric_cols,\n",
    "    columns=[f\"PC{i+1}\" for i in range(k)]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Loadings matrix (features x PCs) – head ===\")\n",
    "display(loadings.head())\n",
    "\n",
    "# Top features per PC (by absolute loading)\n",
    "N = min(8, k)  # show for PC1..PC8\n",
    "for i in range(N):\n",
    "    pc_name = f\"PC{i+1}\"\n",
    "    print(f\"\\n=== Top 10 features for {pc_name} (by |loading|) ===\")\n",
    "    top_idx = (\n",
    "        loadings[pc_name]\n",
    "        .abs()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(10)\n",
    "        .index\n",
    "    )\n",
    "    top_df = pd.DataFrame({\n",
    "        \"feature\": top_idx,\n",
    "        \"loading\": loadings.loc[top_idx, pc_name],\n",
    "        \"abs_loading\": loadings.loc[top_idx, pc_name].abs()\n",
    "    })\n",
    "    display(top_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e67e40b-7893-4db4-8198-2b26ddfd151d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5.1: Feature Domination in each PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9f88823-eb1f-4db1-9814-9c6c0e6c1085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Choose numeric columns & exclude labels / IDs\n",
    "# -----------------------------\n",
    "numeric_cols = [\n",
    "    f.name for f in fv_blc_labeled.schema.fields\n",
    "    if isinstance(f.dataType, NumericType)\n",
    "]\n",
    "\n",
    "cols_to_exclude = [\n",
    "    \"current_roll\",\n",
    "    \"roll1_flag\",\n",
    "    \"roll2_flag\",\n",
    "    \"Metering Omega Pitch\",\n",
    "    \"Set Speed\",\n",
    "    \"is_ts_in_blc_stop\",\n",
    "    \"time_since_failure\",\n",
    "    \"time_to_failure\",\n",
    "]\n",
    "\n",
    "numeric_cols = [c for c in numeric_cols if c not in cols_to_exclude]\n",
    "\n",
    "print(f\"Total numeric columns used for PCA: {len(numeric_cols)}\")\n",
    "print(\"Sample of numeric columns:\", numeric_cols[:15])\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Move data to pandas & scale\n",
    "# -----------------------------\n",
    "# drop rows with any NaN in PCA columns\n",
    "df_pd = (\n",
    "    fv_blc_labeled\n",
    "    .select(numeric_cols)\n",
    "    .dropna(subset=numeric_cols)\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "print(\"Rows going into PCA (Spark -> pandas):\", df_pd.shape[0])\n",
    "print(\"Shape of pandas matrix:\", df_pd.shape)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_pd.values)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Run PCA\n",
    "# -----------------------------\n",
    "k = min(20, X_scaled.shape[1])  # up to 20 PCs or #features\n",
    "pca = PCA(n_components=k, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "eigenvalues = pca.explained_variance_\n",
    "expl_var_ratio = pca.explained_variance_ratio_\n",
    "cum_ratio = np.cumsum(expl_var_ratio)\n",
    "\n",
    "# Table of eigenvalues & explained variance\n",
    "eig_df = pd.DataFrame({\n",
    "    \"PC\": [f\"PC{i+1}\" for i in range(k)],\n",
    "    \"eigenvalue\": eigenvalues,\n",
    "    \"expl_var_ratio\": expl_var_ratio,\n",
    "    \"cum_expl_var_ratio\": cum_ratio,\n",
    "})\n",
    "\n",
    "print(\"\\n=== Eigenvalues & explained variance ===\")\n",
    "display(eig_df)\n",
    "\n",
    "# How many PCs for >=70% variance?\n",
    "num_for_70 = int(np.argmax(cum_ratio >= 0.70) + 1)\n",
    "print(f\"\\nNumber of PCs needed to explain ≥70% variance: {num_for_70}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Scree plot\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, k+1), expl_var_ratio, marker=\"o\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Optional: vertical line at 70% cutoff\n",
    "plt.axvline(num_for_70, linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Feature loadings (which features dominate each PC)\n",
    "# -----------------------------\n",
    "components = pca.components_  # shape (k, n_features)\n",
    "loadings = pd.DataFrame(\n",
    "    components.T,\n",
    "    index=numeric_cols,\n",
    "    columns=[f\"PC{i+1}\" for i in range(k)]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Loadings matrix (features x PCs) – head ===\")\n",
    "display(loadings.head())\n",
    "\n",
    "# Top features per PC (by absolute loading)\n",
    "N = min(8, k)  # show for PC1..PC8\n",
    "for i in range(N):\n",
    "    pc_name = f\"PC{i+1}\"\n",
    "    print(f\"\\n=== Top 10 features for {pc_name} (by |loading|) ===\")\n",
    "    top_idx = (\n",
    "        loadings[pc_name]\n",
    "        .abs()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(10)\n",
    "        .index\n",
    "    )\n",
    "    top_df = pd.DataFrame({\n",
    "        \"feature\": top_idx,\n",
    "        \"loading\": loadings.loc[top_idx, pc_name],\n",
    "        \"abs_loading\": loadings.loc[top_idx, pc_name].abs()\n",
    "    })\n",
    "    display(top_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dd21353-e31d-46fc-b411-70468b4b17e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5.2 PCA Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9118ce9-33fa-4126-b441-7b7862271488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# =========================================================\n",
    "# Put PCA results into dataframe\n",
    "# =========================================================\n",
    "pca_df = pd.DataFrame({\n",
    "    \"PC1\": X_pca[:,0],\n",
    "    \"PC2\": X_pca[:,1],\n",
    "    \"PC3\": X_pca[:,2],\n",
    "    \"label\": df_pd_plot[\"label\"]\n",
    "})\n",
    "\n",
    "# =========================================================\n",
    "# Adjustable typography controls (EDIT FREELY)\n",
    "# =========================================================\n",
    "AXIS_LABEL_SIZE   = 16\n",
    "AXIS_LABEL_WEIGHT = \"bold\"\n",
    "\n",
    "TICK_LABEL_SIZE   = 13\n",
    "\n",
    "TITLE_SIZE        = 18\n",
    "TITLE_WEIGHT      = \"bold\"\n",
    "\n",
    "LEGEND_SIZE       = 12\n",
    "\n",
    "MARKER_SIZE       = 8\n",
    "ALPHA             = 0.75\n",
    "\n",
    "# =========================================================\n",
    "# Color palette (UNCHANGED)\n",
    "# =========================================================\n",
    "palette = {\n",
    "    \"Normal\": \"lightgrey\",\n",
    "    \"Speed = 0\": \"#1f77b4\",\n",
    "    \"BLC Stop\": \"#d62728\",\n",
    "    \"Ramp Up/Down\": \"#ff7f0e\",\n",
    "    \"Inf time_since_failure\": \"#9467bd\",\n",
    "    \"Inf time_to_failure\": \"#8c564b\",\n",
    "    \"Splice Window\": \"#2ca02c\",\n",
    "    \"Stop-Merged\": \"#e377c2\"\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# Scatter plot function (UNCHANGED logic, typography added)\n",
    "# =========================================================\n",
    "def plot_pca(x, y, title, filename):\n",
    "    fig, ax = plt.subplots(figsize=(11.5, 7.5))  # BIG, thesis-scale\n",
    "\n",
    "    sns.scatterplot(\n",
    "        ax=ax,\n",
    "        data=pca_df,\n",
    "        x=x, y=y,\n",
    "        hue=\"label\",\n",
    "        palette=palette,\n",
    "        s=MARKER_SIZE,\n",
    "        alpha=ALPHA,\n",
    "        edgecolor=None\n",
    "    )\n",
    "\n",
    "    # Axis labels\n",
    "    ax.set_xlabel(\n",
    "        x,\n",
    "        fontsize=AXIS_LABEL_SIZE,\n",
    "        fontweight=AXIS_LABEL_WEIGHT,\n",
    "        labelpad=14\n",
    "    )\n",
    "    ax.set_ylabel(\n",
    "        y,\n",
    "        fontsize=AXIS_LABEL_SIZE,\n",
    "        fontweight=AXIS_LABEL_WEIGHT,\n",
    "        labelpad=14\n",
    "    )\n",
    "\n",
    "    # Tick values\n",
    "    ax.tick_params(axis=\"both\", labelsize=TICK_LABEL_SIZE)\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(\n",
    "        title,\n",
    "        fontsize=TITLE_SIZE,\n",
    "        fontweight=TITLE_WEIGHT,\n",
    "        pad=16\n",
    "    )\n",
    "\n",
    "    # Legend\n",
    "    legend = ax.legend(\n",
    "        loc=\"best\",\n",
    "        fontsize=LEGEND_SIZE,\n",
    "        frameon=True,\n",
    "        framealpha=0.95\n",
    "    )\n",
    "\n",
    "    # ---- FIX: legend color visibility ----\n",
    "    for handle in legend.legendHandles:\n",
    "        handle.set_alpha(1.0)\n",
    "        handle.set_markersize(8)\n",
    "\n",
    "    # Layout + export\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{filename}.pdf\", bbox_inches=\"tight\")\n",
    "    fig.savefig(f\"{filename}.png\", dpi=400, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3 SEPARATE HIGH-QUALITY PLOTS\n",
    "# =========================================================\n",
    "plot_pca(\"PC1\", \"PC2\", \"PCA Scatter for  Reduced Feature Dataset: PC1 vs PC2\", \"pca_pc1_pc2\")\n",
    "plot_pca(\"PC2\", \"PC3\", \"PCA Scatter for Reduced Feature Dataset: PC2 vs PC3\", \"pca_pc2_pc3\")\n",
    "plot_pca(\"PC1\", \"PC3\", \"PCA Scatter for Reduced Feature Dataset: PC1 vs PC3\", \"pca_pc1_pc3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5573f60-2a2b-4b25-809b-3959e832e18e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 6: PCA Setup (Small Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfbb8f5e-4c5b-4f99-aa6a-b50e4c9c74e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1) Select numeric columns from your Spark DF\n",
    "numeric_cols = [\n",
    "    f.name for f in fv_blc_labeled.schema.fields\n",
    "    if isinstance(f.dataType, NumericType)\n",
    "]\n",
    "\n",
    "# 2) Explicitly drop columns you *don’t* want in PCA\n",
    "cols_to_exclude = [\n",
    "    \"current_roll\", \"roll1_flag\", \"roll2_flag\",\n",
    "    \"Metering Omega Pitch\",\n",
    "    \"is_ts_in_blc_stop\", \"time_since_failure\", \"time_to_failure\",\n",
    "    \"Set Speed\",\n",
    "]\n",
    "\n",
    "numeric_cols = [c for c in numeric_cols if c not in cols_to_exclude]\n",
    "\n",
    "# 3) Drop all 5m and 10m rolling features, keep only 3m + base signals\n",
    "numeric_cols = [\n",
    "    c for c in numeric_cols\n",
    "    if (\"5m\" not in c and \"10m\" not in c)\n",
    "]\n",
    "\n",
    "print(f\"Total numeric columns used for PCA: {len(numeric_cols)}\")\n",
    "print(\"Sample of numeric columns:\", numeric_cols[:15])\n",
    "\n",
    "# 4) Drop rows with NULLs in PCA columns\n",
    "df_for_pca = fv_blc_labeled.dropna(subset=numeric_cols)\n",
    "print(\"Rows going into PCA (Spark):\", df_for_pca.count())\n",
    "\n",
    "# --- OPTIONAL: sample if you’re worried about memory / speed ---\n",
    "# df_for_pca = df_for_pca.sample(fraction=0.3, seed=42)\n",
    "\n",
    "# 5) Bring only the PCA columns to pandas\n",
    "pdf = df_for_pca.select(numeric_cols).toPandas()\n",
    "print(\"Shape of pandas matrix:\", pdf.shape)  # (rows, features)\n",
    "\n",
    "# 6) Standardize: zero mean, unit variance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(pdf.values)\n",
    "\n",
    "# 7) Run PCA – up to 20 components or #features, whichever is smaller\n",
    "k = min(20, X_scaled.shape[1])\n",
    "pca = PCA(n_components=k)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained = pca.explained_variance_ratio_\n",
    "cum_explained = np.cumsum(explained)\n",
    "\n",
    "print(\"\\n=== Per-component and cumulative explained variance ===\")\n",
    "for i, (var, cum) in enumerate(zip(explained, cum_explained), start=1):\n",
    "    print(f\"PC{i:2d}: var = {var:.4f},  cum = {cum:.4f}\")\n",
    "\n",
    "# 8) How many PCs to reach ≥70% variance?\n",
    "num_for_70 = int(np.argmax(cum_explained >= 0.70) + 1)\n",
    "print(f\"\\nNumber of PCs needed to explain ≥70% variance: {num_for_70}\")\n",
    "\n",
    "# 9) (Optional) small result DataFrame for the first few PCs\n",
    "pca_summary = pd.DataFrame({\n",
    "    \"PC\": [f\"PC{i}\" for i in range(1, k+1)],\n",
    "    \"explained_variance_ratio\": explained,\n",
    "    \"cumulative_variance_ratio\": cum_explained\n",
    "})\n",
    "pca_summary.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "380a0627-f7cc-4b43-a1dd-026efe3771d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6.1: Feature Domination in each PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e936b9c-a437-4297-a049-54c696edba61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Choose numeric columns & exclude labels / IDs\n",
    "# -----------------------------\n",
    "numeric_cols = [\n",
    "    f.name for f in fv_blc_labeled.schema.fields\n",
    "    if isinstance(f.dataType, NumericType)\n",
    "]\n",
    "\n",
    "cols_to_exclude = [\n",
    "    \"current_roll\",\n",
    "    \"roll1_flag\",\n",
    "    \"roll2_flag\",\n",
    "    \"Metering Omega Pitch\",\n",
    "    \"Set Speed\",\n",
    "    \"is_ts_in_blc_stop\",\n",
    "    \"time_since_failure\",\n",
    "    \"time_to_failure\",\n",
    "]\n",
    "\n",
    "numeric_cols = [c for c in numeric_cols if c not in cols_to_exclude]\n",
    "\n",
    "# Keep only base + 3m features (drop 5m and 10m rollings)\n",
    "numeric_cols = [\n",
    "    c for c in numeric_cols\n",
    "    if (\"5m\" not in c and \"10m\" not in c)\n",
    "]\n",
    "\n",
    "print(f\"Total numeric columns used for PCA: {len(numeric_cols)}\")\n",
    "print(\"Sample of numeric columns:\", numeric_cols[:15])\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Move data to pandas & scale\n",
    "# -----------------------------\n",
    "df_pd = (\n",
    "    fv_blc_labeled\n",
    "    .select(numeric_cols)\n",
    "    .dropna(subset=numeric_cols)\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "print(\"Rows going into PCA (Spark -> pandas):\", df_pd.shape[0])\n",
    "print(\"Shape of pandas matrix:\", df_pd.shape)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_pd.values)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Run PCA\n",
    "# -----------------------------\n",
    "k = min(20, X_scaled.shape[1])  # up to 20 PCs or #features\n",
    "pca = PCA(n_components=k, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "eigenvalues = pca.explained_variance_\n",
    "expl_var_ratio = pca.explained_variance_ratio_\n",
    "cum_ratio = np.cumsum(expl_var_ratio)\n",
    "\n",
    "eig_df = pd.DataFrame({\n",
    "    \"PC\": [f\"PC{i+1}\" for i in range(k)],\n",
    "    \"eigenvalue\": eigenvalues,\n",
    "    \"expl_var_ratio\": expl_var_ratio,\n",
    "    \"cum_expl_var_ratio\": cum_ratio,\n",
    "})\n",
    "\n",
    "print(\"\\n=== Eigenvalues & explained variance ===\")\n",
    "display(eig_df)\n",
    "\n",
    "num_for_70 = int(np.argmax(cum_ratio >= 0.70) + 1)\n",
    "print(f\"\\nNumber of PCs needed to explain ≥70% variance: {num_for_70}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Scree plot\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, k+1), expl_var_ratio, marker=\"o\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.grid(True)\n",
    "plt.axvline(num_for_70, linestyle=\"--\")  # 70% cutoff\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Feature loadings (which features dominate each PC)\n",
    "# -----------------------------\n",
    "components = pca.components_  # shape (k, n_features)\n",
    "loadings = pd.DataFrame(\n",
    "    components.T,\n",
    "    index=numeric_cols,\n",
    "    columns=[f\"PC{i+1}\" for i in range(k)]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Loadings matrix (features x PCs) – head ===\")\n",
    "display(loadings.head())\n",
    "\n",
    "# Top features per PC (by absolute loading)\n",
    "N = min(8, k)  # show for PC1..PC8\n",
    "for i in range(N):\n",
    "    pc_name = f\"PC{i+1}\"\n",
    "    print(f\"\\n=== Top 10 features for {pc_name} (by |loading|) ===\")\n",
    "    top_idx = (\n",
    "        loadings[pc_name]\n",
    "        .abs()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(10)\n",
    "        .index\n",
    "    )\n",
    "    top_df = pd.DataFrame({\n",
    "        \"feature\": top_idx,\n",
    "        \"loading\": loadings.loc[top_idx, pc_name],\n",
    "        \"abs_loading\": loadings.loc[top_idx, pc_name].abs()\n",
    "    })\n",
    "    display(top_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b55812f-61db-437e-b73c-8c4b76b93337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6.2: PCA Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb3e115a-1923-44ea-a0c9-62c33ebf7250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# =========================================================\n",
    "# Put PCA results into dataframe\n",
    "# =========================================================\n",
    "pca_df = pd.DataFrame({\n",
    "    \"PC1\": X_pca[:,0],\n",
    "    \"PC2\": X_pca[:,1],\n",
    "    \"PC3\": X_pca[:,2],\n",
    "    \"label\": df_pd_plot[\"label\"]\n",
    "})\n",
    "\n",
    "# =========================================================\n",
    "# Adjustable typography controls (EDIT FREELY)\n",
    "# =========================================================\n",
    "AXIS_LABEL_SIZE   = 16\n",
    "AXIS_LABEL_WEIGHT = \"bold\"\n",
    "\n",
    "TICK_LABEL_SIZE   = 13\n",
    "\n",
    "TITLE_SIZE        = 18\n",
    "TITLE_WEIGHT      = \"bold\"\n",
    "\n",
    "LEGEND_SIZE       = 12\n",
    "\n",
    "MARKER_SIZE       = 8\n",
    "ALPHA             = 0.75\n",
    "\n",
    "# =========================================================\n",
    "# Color palette (UNCHANGED)\n",
    "# =========================================================\n",
    "palette = {\n",
    "    \"Normal\": \"lightgrey\",\n",
    "    \"Speed = 0\": \"#1f77b4\",\n",
    "    \"BLC Stop\": \"#d62728\",\n",
    "    \"Ramp Up/Down\": \"#ff7f0e\",\n",
    "    \"Inf time_since_failure\": \"#9467bd\",\n",
    "    \"Inf time_to_failure\": \"#8c564b\",\n",
    "    \"Splice Window\": \"#2ca02c\",\n",
    "    \"Stop-Merged\": \"#e377c2\"\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# Scatter plot function (UNCHANGED logic, typography added)\n",
    "# =========================================================\n",
    "def plot_pca(x, y, title, filename):\n",
    "    fig, ax = plt.subplots(figsize=(11.5, 7.5))  # BIG, thesis-scale\n",
    "\n",
    "    sns.scatterplot(\n",
    "        ax=ax,\n",
    "        data=pca_df,\n",
    "        x=x, y=y,\n",
    "        hue=\"label\",\n",
    "        palette=palette,\n",
    "        s=MARKER_SIZE,\n",
    "        alpha=ALPHA,\n",
    "        edgecolor=None\n",
    "    )\n",
    "\n",
    "    # Axis labels\n",
    "    ax.set_xlabel(\n",
    "        x,\n",
    "        fontsize=AXIS_LABEL_SIZE,\n",
    "        fontweight=AXIS_LABEL_WEIGHT,\n",
    "        labelpad=14\n",
    "    )\n",
    "    ax.set_ylabel(\n",
    "        y,\n",
    "        fontsize=AXIS_LABEL_SIZE,\n",
    "        fontweight=AXIS_LABEL_WEIGHT,\n",
    "        labelpad=14\n",
    "    )\n",
    "\n",
    "    # Tick values\n",
    "    ax.tick_params(axis=\"both\", labelsize=TICK_LABEL_SIZE)\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(\n",
    "        title,\n",
    "        fontsize=TITLE_SIZE,\n",
    "        fontweight=TITLE_WEIGHT,\n",
    "        pad=16\n",
    "    )\n",
    "\n",
    "    # Legend\n",
    "    legend = ax.legend(\n",
    "        loc=\"best\",\n",
    "        fontsize=LEGEND_SIZE,\n",
    "        frameon=True,\n",
    "        framealpha=0.95\n",
    "    )\n",
    "\n",
    "    # ---- FIX: legend color visibility ----\n",
    "    for handle in legend.legendHandles:\n",
    "        handle.set_alpha(1.0)\n",
    "        handle.set_markersize(8)\n",
    "\n",
    "    # Layout + export\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{filename}.pdf\", bbox_inches=\"tight\")\n",
    "    fig.savefig(f\"{filename}.png\", dpi=400, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3 SEPARATE HIGH-QUALITY PLOTS\n",
    "# =========================================================\n",
    "plot_pca(\"PC1\", \"PC2\", \"PCA Scatter for  Reduced Feature Dataset: PC1 vs PC2\", \"pca_pc1_pc2\")\n",
    "plot_pca(\"PC2\", \"PC3\", \"PCA Scatter for Reduced Feature Dataset: PC2 vs PC3\", \"pca_pc2_pc3\")\n",
    "plot_pca(\"PC1\", \"PC3\", \"PCA Scatter for Reduced Feature Dataset: PC1 vs PC3\", \"pca_pc1_pc3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30989f55-57b9-442f-8266-999e58f2ffe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6.3: PCA Plosts with Smaller Regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f218dc2-c37c-4950-af58-e120a61cc30a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 0) Imports\n",
    "# =========================================================\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1) FILTER TO ONLY LAST 120 MIN BEFORE STOP\n",
    "# =========================================================\n",
    "df_filtered = fv_blc_labeled.filter(\n",
    "    (F.col(\"time_to_failure\") <= 120) | (F.col(\"is_ts_in_blc_stop\") == 1)\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) APPLY LABEL LOGIC (UNCHANGED)\n",
    "# =========================================================\n",
    "df_filtered = df_filtered.withColumn(\n",
    "    \"label\",\n",
    "    F.when(F.col(\"is_ts_in_blc_stop\") == 1, \"BLC Stop\")\n",
    "     .when(F.col(\"Current Speed\") == 0, \"Speed = 0\")\n",
    "     .when(\n",
    "         (F.col(\"Current Speed\") != F.col(\"Set Speed\")) &\n",
    "         (F.col(\"Current Speed\") != 0),\n",
    "         \"Ramp Up/Down\"\n",
    "     )\n",
    "     .when(F.col(\"splice_flag\") == 1, \"Splice Window\")\n",
    "     .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) MOVE TO PANDAS (ALIGN WITH PCA BASE)\n",
    "# =========================================================\n",
    "df_pd_filtered = (\n",
    "    df_filtered\n",
    "    .select(numeric_cols + [\"label\"])\n",
    "    .dropna()\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4) APPLY EXISTING SCALER + PCA (NO REFIT)\n",
    "# =========================================================\n",
    "X_scaled_filtered = scaler.transform(\n",
    "    df_pd_filtered[numeric_cols].values\n",
    ")\n",
    "\n",
    "X_pca_filtered = pca.transform(X_scaled_filtered)\n",
    "\n",
    "df_pd_filtered[\"PC1\"] = X_pca_filtered[:, 0]\n",
    "df_pd_filtered[\"PC2\"] = X_pca_filtered[:, 1]\n",
    "df_pd_filtered[\"PC3\"] = X_pca_filtered[:, 2]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5) PLOTTING PARAMETERS (ADJUST FREELY)\n",
    "# =========================================================\n",
    "AXIS_LABEL_SIZE   = 16\n",
    "AXIS_LABEL_WEIGHT = \"bold\"\n",
    "\n",
    "TICK_LABEL_SIZE   = 13\n",
    "\n",
    "TITLE_SIZE        = 18\n",
    "TITLE_WEIGHT      = \"bold\"\n",
    "\n",
    "LEGEND_SIZE       = 12\n",
    "\n",
    "MARKER_SIZE       = 12\n",
    "ALPHA             = 0.85\n",
    "\n",
    "\n",
    "labels = [\"Normal\", \"Splice Window\", \"Speed = 0\", \"Ramp Up/Down\", \"BLC Stop\"]\n",
    "\n",
    "colors = {\n",
    "    \"Normal\": \"#d3d3d3\",\n",
    "    \"Splice Window\": \"#1f77b4\",\n",
    "    \"Speed = 0\": \"#2ca02c\",\n",
    "    \"Ramp Up/Down\": \"#ff7f0e\",\n",
    "    \"BLC Stop\": \"#d62728\",\n",
    "}\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6) HELPER: SINGLE PCA SCATTER PLOT\n",
    "# =========================================================\n",
    "def plot_pca_filtered(xcol, ycol, filename):\n",
    "    fig, ax = plt.subplots(figsize=(11.5, 7.5))\n",
    "\n",
    "    for lbl in labels:\n",
    "        subset = df_pd_filtered[df_pd_filtered[\"label\"] == lbl]\n",
    "        ax.scatter(\n",
    "            subset[xcol],\n",
    "            subset[ycol],\n",
    "            s=MARKER_SIZE,\n",
    "            alpha=ALPHA,\n",
    "            color=colors[lbl],\n",
    "            label=lbl\n",
    "        )\n",
    "\n",
    "    # Axis labels\n",
    "    ax.set_xlabel(\n",
    "        xcol,\n",
    "        fontsize=AXIS_LABEL_SIZE,\n",
    "        fontweight=AXIS_LABEL_WEIGHT,\n",
    "        labelpad=14\n",
    "    )\n",
    "    ax.set_ylabel(\n",
    "        ycol,\n",
    "        fontsize=AXIS_LABEL_SIZE,\n",
    "        fontweight=AXIS_LABEL_WEIGHT,\n",
    "        labelpad=14\n",
    "    )\n",
    "\n",
    "    # Tick labels\n",
    "    ax.tick_params(axis=\"both\", labelsize=TICK_LABEL_SIZE)\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(\n",
    "        f\"PCA Scatter for Reduced Feature Dataset: {xcol} vs {ycol}\",\n",
    "        fontsize=TITLE_SIZE,\n",
    "        fontweight=TITLE_WEIGHT,\n",
    "        pad=16\n",
    "    )\n",
    "\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Legend\n",
    "    legend = ax.legend(\n",
    "        loc=\"best\",\n",
    "        fontsize=LEGEND_SIZE,\n",
    "        frameon=True,\n",
    "        framealpha=0.95\n",
    "    )\n",
    "\n",
    "    # Ensure legend visibility\n",
    "    for handle in legend.legendHandles:\n",
    "        handle.set_alpha(1.0)\n",
    "\n",
    "\n",
    "    # Export\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{filename}.pdf\", bbox_inches=\"tight\")\n",
    "    fig.savefig(f\"{filename}.png\", dpi=400, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7) GENERATE 3 SEPARATE HIGH-QUALITY FIGURES\n",
    "# =========================================================\n",
    "plot_pca_filtered(\"PC1\", \"PC2\", \"pca_filtered_pc1_pc2\")\n",
    "plot_pca_filtered(\"PC2\", \"PC3\", \"pca_filtered_pc2_pc3\")\n",
    "plot_pca_filtered(\"PC1\", \"PC3\", \"pca_filtered_pc1_pc3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55e35995-a713-401c-ae21-bb3caaf820f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 7: PCA Setup (Mini Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6c5bc0a-3ad4-4b4d-8510-ee2070141c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 0) Restrict to smaller regime:\n",
    "#    - points within 120 min to failure, OR\n",
    "#    - points that are actually in BLC stop\n",
    "df_filtered = fv_blc_labeled.filter(\n",
    "    (F.col(\"time_to_failure\") <= 120) | (F.col(\"is_ts_in_blc_stop\") == 1)\n",
    ")\n",
    "\n",
    "print(\"Rows in filtered window (<=120 min or BLC stop):\", df_filtered.count())\n",
    "\n",
    "# 1) Select numeric columns from the FILTERED DF\n",
    "numeric_cols = [\n",
    "    f.name for f in df_filtered.schema.fields\n",
    "    if isinstance(f.dataType, NumericType)\n",
    "]\n",
    "\n",
    "# 2) Explicitly drop columns you don’t want in PCA\n",
    "cols_to_exclude = [\n",
    "    \"current_roll\", \"roll1_flag\", \"roll2_flag\",\n",
    "    \"Metering Omega Pitch\",\n",
    "    \"is_ts_in_blc_stop\", \"time_since_failure\", \"time_to_failure\",\n",
    "    \"Set Speed\",\n",
    "]\n",
    "\n",
    "numeric_cols = [c for c in numeric_cols if c not in cols_to_exclude]\n",
    "\n",
    "# 3) Drop all 5m and 10m rolling features, keep only 3m + base signals\n",
    "numeric_cols = [\n",
    "    c for c in numeric_cols\n",
    "    if (\"5m\" not in c and \"10m\" not in c)\n",
    "]\n",
    "\n",
    "print(f\"Total numeric columns used for PCA: {len(numeric_cols)}\")\n",
    "print(\"Sample of numeric columns:\", numeric_cols[:15])\n",
    "\n",
    "# 4) Drop rows with NULLs in PCA columns\n",
    "df_for_pca = df_filtered.dropna(subset=numeric_cols)\n",
    "print(\"Rows going into PCA (Spark):\", df_for_pca.count())\n",
    "\n",
    "# 5) Bring only the PCA columns to pandas\n",
    "pdf = df_for_pca.select(numeric_cols).toPandas()\n",
    "print(\"Shape of pandas matrix:\", pdf.shape)  # (rows, features)\n",
    "\n",
    "# 6) Standardize: zero mean, unit variance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(pdf.values)\n",
    "\n",
    "# 7) Run PCA – up to 20 components or #features, whichever is smaller\n",
    "k = min(20, X_scaled.shape[1])\n",
    "pca = PCA(n_components=k, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained = pca.explained_variance_ratio_\n",
    "cum_explained = np.cumsum(explained)\n",
    "\n",
    "print(\"\\n=== Per-component and cumulative explained variance ===\")\n",
    "for i, (var, cum) in enumerate(zip(explained, cum_explained), start=1):\n",
    "    print(f\"PC{i:2d}: var = {var:.4f},  cum = {cum:.4f}\")\n",
    "\n",
    "# 8) How many PCs to reach ≥70% variance?\n",
    "num_for_70 = int(np.argmax(cum_explained >= 0.70) + 1)\n",
    "print(f\"\\nNumber of PCs needed to explain ≥70% variance: {num_for_70}\")\n",
    "\n",
    "# 9) (Optional) summary DF\n",
    "pca_summary = pd.DataFrame({\n",
    "    \"PC\": [f\"PC{i}\" for i in range(1, k+1)],\n",
    "    \"explained_variance_ratio\": explained,\n",
    "    \"cumulative_variance_ratio\": cum_explained\n",
    "})\n",
    "pca_summary.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da682a9a-a2ec-4ecc-9787-685015cfe1ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7.1: Feature Domination in each PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd3c3057-e4a0-4cf6-b12a-9ee1ad421b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Filter to smaller regime\n",
    "# -----------------------------\n",
    "df_filtered = fv_blc_labeled.filter(\n",
    "    (F.col(\"time_to_failure\") <= 120) | (F.col(\"is_ts_in_blc_stop\") == 1)\n",
    ")\n",
    "\n",
    "print(\"Rows in filtered window (<=120 min or BLC stop):\", df_filtered.count())\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Choose numeric columns & exclude labels / IDs\n",
    "# -----------------------------\n",
    "numeric_cols = [\n",
    "    f.name for f in df_filtered.schema.fields\n",
    "    if isinstance(f.dataType, NumericType)\n",
    "]\n",
    "\n",
    "cols_to_exclude = [\n",
    "    \"current_roll\",\n",
    "    \"roll1_flag\",\n",
    "    \"roll2_flag\",\n",
    "    \"Metering Omega Pitch\",\n",
    "    \"Set Speed\",\n",
    "    \"is_ts_in_blc_stop\",\n",
    "    \"time_since_failure\",\n",
    "    \"time_to_failure\",\n",
    "]\n",
    "\n",
    "numeric_cols = [c for c in numeric_cols if c not in cols_to_exclude]\n",
    "\n",
    "# Keep only base + 3m features (drop 5m and 10m rolling features)\n",
    "numeric_cols = [\n",
    "    c for c in numeric_cols\n",
    "    if (\"5m\" not in c and \"10m\" not in c)\n",
    "]\n",
    "\n",
    "print(f\"Total numeric columns used for PCA: {len(numeric_cols)}\")\n",
    "print(\"Sample of numeric columns:\", numeric_cols[:15])\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Move data to pandas & scale\n",
    "# -----------------------------\n",
    "df_pd = (\n",
    "    df_filtered\n",
    "    .select(numeric_cols)\n",
    "    .dropna(subset=numeric_cols)\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "print(\"Rows going into PCA (Spark -> pandas):\", df_pd.shape[0])\n",
    "print(\"Shape of pandas matrix:\", df_pd.shape)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_pd.values)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Run PCA\n",
    "# -----------------------------\n",
    "k = min(20, X_scaled.shape[1])  # up to 20 PCs or #features\n",
    "pca = PCA(n_components=k, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "eigenvalues = pca.explained_variance_\n",
    "expl_var_ratio = pca.explained_variance_ratio_\n",
    "cum_ratio = np.cumsum(expl_var_ratio)\n",
    "\n",
    "eig_df = pd.DataFrame({\n",
    "    \"PC\": [f\"PC{i+1}\" for i in range(k)],\n",
    "    \"eigenvalue\": eigenvalues,\n",
    "    \"expl_var_ratio\": expl_var_ratio,\n",
    "    \"cum_expl_var_ratio\": cum_ratio,\n",
    "})\n",
    "\n",
    "print(\"\\n=== Eigenvalues & explained variance ===\")\n",
    "display(eig_df)\n",
    "\n",
    "num_for_70 = int(np.argmax(cum_ratio >= 0.70) + 1)\n",
    "print(f\"\\nNumber of PCs needed to explain ≥70% variance: {num_for_70}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Scree plot\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, k+1), expl_var_ratio, marker=\"o\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.title(\"Scree Plot (Filtered ≤120 min / BLC stop, 3m features)\")\n",
    "plt.grid(True)\n",
    "plt.axvline(num_for_70, linestyle=\"--\")  # 70% cutoff\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Feature loadings (which features dominate each PC)\n",
    "# -----------------------------\n",
    "components = pca.components_  # shape (k, n_features)\n",
    "loadings = pd.DataFrame(\n",
    "    components.T,\n",
    "    index=numeric_cols,\n",
    "    columns=[f\"PC{i+1}\" for i in range(k)]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Loadings matrix (features x PCs) – head ===\")\n",
    "display(loadings.head())\n",
    "\n",
    "# Keep X_pca + df_pd for later plotting\n",
    "# (PC1, PC2, PC3 are X_pca[:,0], X_pca[:,1], X_pca[:,2])\n",
    "\n",
    "# Top features per PC (by absolute loading)\n",
    "N = min(8, k)  # show for PC1..PC8\n",
    "for i in range(N):\n",
    "    pc_name = f\"PC{i+1}\"\n",
    "    print(f\"\\n=== Top 10 features for {pc_name} (by |loading|) ===\")\n",
    "    top_idx = (\n",
    "        loadings[pc_name]\n",
    "        .abs()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(10)\n",
    "        .index\n",
    "    )\n",
    "    top_df = pd.DataFrame({\n",
    "        \"feature\": top_idx,\n",
    "        \"loading\": loadings.loc[top_idx, pc_name],\n",
    "        \"abs_loading\": loadings.loc[top_idx, pc_name].abs()\n",
    "    })\n",
    "    display(top_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b51d46c2-dc55-497b-a492-630cfafd62c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7.2: PCA Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7be41db5-2aeb-472f-b532-ed184749485b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import NumericType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1) FILTER REGIME (<=120 min OR in BLC stop)\n",
    "# ----------------------------------------------------------\n",
    "df_filtered = fv_blc_labeled.filter(\n",
    "    (F.col(\"time_to_failure\") <= 120) | (F.col(\"is_ts_in_blc_stop\") == 1)\n",
    ")\n",
    "\n",
    "print(\"Rows in filtered DF:\", df_filtered.count())\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2) ADD LABEL COLUMN HERE (ON THE SAME FILTERED SET)\n",
    "# ----------------------------------------------------------\n",
    "df_labeled = df_filtered.withColumn(\n",
    "    \"label\",\n",
    "    F.when(F.col(\"is_ts_in_blc_stop\") == 1, \"BLC Stop\")\n",
    "     #.when(F.col(\"Current Speed\") == 0, \"Speed = 0\")\n",
    "     .when(\n",
    "         (F.col(\"Current Speed\") != F.col(\"Set Speed\")) &\n",
    "         (F.col(\"Current Speed\") != 0),\n",
    "         \"Ramp Up/Down\"\n",
    "     )\n",
    "     .when(F.col(\"splice_flag\") == 1, \"Splice Window\")\n",
    "     .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3) SELECT NUMERIC FEATURES (base + 3m only)\n",
    "# ----------------------------------------------------------\n",
    "numeric_cols = [\n",
    "    f.name for f in df_labeled.schema.fields\n",
    "    if isinstance(f.dataType, NumericType)\n",
    "]\n",
    "\n",
    "cols_to_exclude = [\n",
    "    \"current_roll\", \"roll1_flag\", \"roll2_flag\",\n",
    "    \"Metering Omega Pitch\",\n",
    "    \"Set Speed\",\n",
    "    \"is_ts_in_blc_stop\", \"time_since_failure\", \"time_to_failure\",\n",
    "]\n",
    "\n",
    "numeric_cols = [c for c in numeric_cols if c not in cols_to_exclude]\n",
    "numeric_cols = [c for c in numeric_cols if (\"5m\" not in c and \"10m\" not in c)]\n",
    "\n",
    "print(\"Numeric columns used:\", len(numeric_cols))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4) BRING PCA FEATURES + LABELS INTO ONE PANDAS DF\n",
    "# ----------------------------------------------------------\n",
    "df_pd = (\n",
    "    df_labeled\n",
    "    .select(numeric_cols + [\"label\"])\n",
    "    .dropna(subset=numeric_cols)\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "print(\"Rows in df_pd:\", df_pd.shape[0])\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 5) PCA\n",
    "# ----------------------------------------------------------\n",
    "X = df_pd[numeric_cols].values\n",
    "y = df_pd[\"label\"].values  # ALIGNED!\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "k = min(20, len(numeric_cols))\n",
    "pca = PCA(n_components=k, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 6) Build PCA DF for plotting\n",
    "# ----------------------------------------------------------\n",
    "pca_df = pd.DataFrame({\n",
    "    \"PC1\": X_pca[:, 0],\n",
    "    \"PC2\": X_pca[:, 1],\n",
    "    \"PC3\": X_pca[:, 2],\n",
    "    \"label\": y\n",
    "})\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 7) PLOT\n",
    "# ----------------------------------------------------------\n",
    "palette = {\n",
    "    \"Normal\": \"#B0B0B0\",        # Darker grey for visibility\n",
    "    \"Speed = 0\": \"#0047FF\",     # Deep electric blue\n",
    "    \"BLC Stop\": \"#FF0000\",      # Pure red (high alert)\n",
    "    \"Ramp Up/Down\": \"#FF8C00\",  # Dark orange (vivid)\n",
    "    \"Splice Window\": \"#00CC00\"  # Strong green\n",
    "}\n",
    "\n",
    "\n",
    "def plot(ax, x, y, title):\n",
    "    sns.scatterplot(\n",
    "        data=pca_df,\n",
    "        x=x, y=y,\n",
    "        hue=\"label\",\n",
    "        palette=palette,\n",
    "        s=10, alpha=0.35,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 6))\n",
    "plot(axes[0], \"PC1\", \"PC2\", \"PC1 vs PC2\")\n",
    "plot(axes[1], \"PC2\", \"PC3\", \"PC2 vs PC3\")\n",
    "plot(axes[2], \"PC1\", \"PC3\", \"PC1 vs PC3\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "128d99d3-8fcf-48d3-b2c1-bd39898ca57f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 8: Early Alarm Detection ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb9cdad1-5ca2-4f48-ac71-f39bb2e77631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Simple Supervised Early-Warning Alarm (1ß-Minute Horizon, 0–180 min Regime)\n",
    "\n",
    "**Goal.**  \n",
    "We build a lightweight, supervised alarm model that predicts whether the machine will enter a BLC stop within the next **10 minutes**, using only data from the **0–180 minute regime before a stop**. The focus is on:\n",
    "\n",
    "- Raising alarms shortly before a stop (10-minute horizon)\n",
    "- Keeping the **false positive rate low** (operator trust)\n",
    "- Keeping the model **simple and interpretable**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Label Definition\n",
    "\n",
    "From the existing `time_to_failure` signal, we define a **binary label** for each timestamp:\n",
    "\n",
    "- `y = 1` if `time_to_failure <= 10` minutes  \n",
    "- `y = 0` otherwise  \n",
    "\n",
    "We restrict to rows where `0 <= time_to_failure <= 180` minutes. This creates a focused \"normal regime + pre-stop\" dataset that matches our PCA Phase 3 diagnostics.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Features\n",
    "\n",
    "From all numeric columns, we **exclude**:\n",
    "\n",
    "- Direct labels or leak variables:\n",
    "  - `y`, `time_to_failure`, `time_since_failure`, `is_ts_in_blc_stop`, `splice_flag`\n",
    "- Low-level technical/control fields:\n",
    "  - `current_roll`, `roll1_flag`, `roll2_flag`, `Metering Omega Pitch`, `Set Speed`\n",
    "\n",
    "Everything else (base signals + 3/5/10-minute rolling statistics) is used as input.  \n",
    "This is consistent with the PCA findings, where:\n",
    "\n",
    "- BLC1 loadcell, BLC2 loadcell, and `e_modulus` (and their rolling means/std/slopes) dominated the first components.\n",
    "- Width and middle sensors plus unwinder diameters showed up in later PCs as structural “setup” features.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Model\n",
    "\n",
    "We use a **tree-based classifier** (`HistGradientBoostingClassifier`) because:\n",
    "\n",
    "- It handles non-linear interactions between features (tension, modulus, diameters, etc.).\n",
    "- It does **not** require feature scaling.\n",
    "- It supports **class weights** to handle the strong imbalance between “normal” and “imminent stop” samples.\n",
    "\n",
    "Class weights are set such that positive samples (imminent stop, `y = 1`) are up-weighted relative to the majority normal class.\n",
    "\n",
    "The data is split into:\n",
    "\n",
    "- **Training set:** 70% of the regime data  \n",
    "- **Validation set:** 30% of the regime data (stratified by label)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Threshold Selection (Conservative Alarm)\n",
    "\n",
    "The model outputs a probability `p(stop within 10 min)` for each timestamp.  \n",
    "Instead of using a naive threshold of 0.5, we:\n",
    "\n",
    "1. Compute the **ROC curve** on the validation set.\n",
    "2. Search for a threshold where:\n",
    "   - The **false positive rate (FPR)** is below a small target (e.g. 2%).\n",
    "   - Among these low-FPR points, we pick the one with the **highest true positive rate (recall)**.\n",
    "\n",
    "This gives a **conservative alarm**: it may miss some stops, but when it raises an alarm, it is less likely to be noise.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Evaluation Outputs\n",
    "\n",
    "The code prints:\n",
    "\n",
    "- **ROC AUC**: how well the model separates positive/negative cases in terms of probability.\n",
    "- **Chosen threshold**\n",
    "- **Confusion matrix components**:\n",
    "  - TP: true alarms (correctly predicted imminent stops)\n",
    "  - FP: false alarms (alarm but no stop in 10 min)\n",
    "  - FN: missed alarms (stop but no alarm)\n",
    "  - TN: correct normals (no alarm when there is no stop)\n",
    "\n",
    "It also generates:\n",
    "\n",
    "1. A **ROC curve plot**  \n",
    "   - Model curve vs. random baseline  \n",
    "   - Marked point for the chosen threshold\n",
    "\n",
    "2. A **bar chart** of `[TP, FP, FN, TN]`  \n",
    "   - Clear visual split between correct alarms, false alarms, missed stops, and normal states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "028828a5-9a3e-4920-97be-f70cdbbc7d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Filter to 0–120 min regime and build 3-min label\n",
    "# ---------------------------------------------------------\n",
    "# Keep only rows where we have a defined time_to_failure in [0, 120] minutes\n",
    "df_regime = (\n",
    "    fv_blc_labeled\n",
    "    .filter((F.col(\"time_to_failure\") >= 0) & (F.col(\"time_to_failure\") <= 180))\n",
    ")\n",
    "\n",
    "# Binary label:\n",
    "#   y = 1  if stop will occur within 3 minutes\n",
    "#   y = 0  otherwise\n",
    "df_regime = df_regime.withColumn(\n",
    "    \"y\",\n",
    "    F.when(F.col(\"time_to_failure\") <= 10, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Select numeric feature columns\n",
    "# ---------------------------------------------------------\n",
    "numeric_cols = [\n",
    "    f.name for f in df_regime.schema.fields\n",
    "    if isinstance(f.dataType, NumericType)\n",
    "]\n",
    "\n",
    "# Exclude obvious non-features / targets / leak columns\n",
    "cols_to_exclude = [\n",
    "    \"y\",\n",
    "    \"time_to_failure\",\n",
    "    \"time_since_failure\",\n",
    "    \"is_ts_in_blc_stop\",\n",
    "    \"splice_flag\",\n",
    "    \"current_roll\",\n",
    "    \"roll1_flag\",\n",
    "    \"roll2_flag\",\n",
    "    \"Metering Omega Pitch\",\n",
    "    \"Set Speed\"\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in numeric_cols if c not in cols_to_exclude]\n",
    "\n",
    "print(f\"Number of feature columns: {len(feature_cols)}\")\n",
    "print(\"Sample features:\", feature_cols[:15])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Move regime data to pandas and drop NaNs\n",
    "# ---------------------------------------------------------\n",
    "pdf = (\n",
    "    df_regime\n",
    "    .select(feature_cols + [\"y\"])\n",
    "    .dropna(subset=feature_cols + [\"y\"])\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "print(\"Pandas shape (rows, features+label):\", pdf.shape)\n",
    "\n",
    "X = pdf[feature_cols].values\n",
    "y = pdf[\"y\"].values.astype(int)\n",
    "\n",
    "# Basic label stats\n",
    "n_pos = int(y.sum())\n",
    "n_total = len(y)\n",
    "print(f\"Total samples: {n_total}\")\n",
    "print(f\"Positives (<=10 min to stop): {n_pos} ({n_pos/n_total:.4%})\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) Train / validation split\n",
    "# ---------------------------------------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0], \"Val size:\", X_val.shape[0])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Simple, interpretable classifier (HistGradientBoosting)\n",
    "#    with class weighting to handle imbalance\n",
    "# ---------------------------------------------------------\n",
    "pos_weight = (len(y_train) - y_train.sum()) / max(y_train.sum(), 1)\n",
    "class_weight = {0: 1.0, 1: float(pos_weight)}\n",
    "print(\"Class weights used:\", class_weight)\n",
    "\n",
    "clf = HistGradientBoostingClassifier(\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    max_iter=200,\n",
    "    class_weight=class_weight,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6) Predict probabilities and choose conservative threshold\n",
    "#    – target: low false positive rate (e.g. <= 2%)\n",
    "# ---------------------------------------------------------\n",
    "probs_val = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(y_val, probs_val)\n",
    "print(f\"\\nValidation ROC AUC: {auc:.3f}\")\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, probs_val)\n",
    "\n",
    "# Choose threshold with FPR <= 2% and highest TPR among them\n",
    "target_fpr = 0.02\n",
    "mask = fpr <= target_fpr\n",
    "\n",
    "if mask.any():\n",
    "    idx = np.argmax(tpr[mask])\n",
    "    best_threshold = thresholds[mask][idx]\n",
    "    best_fpr = fpr[mask][idx]\n",
    "    best_tpr = tpr[mask][idx]\n",
    "else:\n",
    "    # If nothing reaches FPR <= 2%, pick the lowest FPR point\n",
    "    idx = np.argmin(fpr)\n",
    "    best_threshold = thresholds[idx]\n",
    "    best_fpr = fpr[idx]\n",
    "    best_tpr = tpr[idx]\n",
    "\n",
    "y_pred = (probs_val >= best_threshold).astype(int)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "fpr_final = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "print(\"\\n==== 10-MIN EARLY WARNING RESULTS (Supervised model, 3 Hour Window) ====\")\n",
    "print(f\"Chosen threshold: {best_threshold:.3f}\")\n",
    "print(f\"TP (true alarms):   {tp}\")\n",
    "print(f\"FP (false alarms):  {fp}\")\n",
    "print(f\"FN (missed alarms): {fn}\")\n",
    "print(f\"TN (correct normals): {tn}\")\n",
    "print(f\"Recall (TPR):   {recall:.3f}\")\n",
    "print(f\"FPR:           {fpr_final:.3f}\")\n",
    "print(f\"Precision:     {precision:.3f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7) Visuals: ROC + Confusion bar chart\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# --- ROC curve plot ---\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC (AUC = {auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\", label=\"Random\")\n",
    "plt.scatter([best_fpr], [best_tpr], color=\"red\", label=\"Chosen threshold\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve – 10-Min Early Warning (T2 regime)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Bar chart: TP vs FP vs FN vs TN ---\n",
    "counts = [tp, fp, fn, tn]\n",
    "labels_bar = [\"TP (True alarms)\", \"FP (False alarms)\", \"FN (Missed)\", \"TN (Correct normals)\"]\n",
    "colors_bar = [\"#2ca02c\", \"#d62728\", \"#ff7f0e\", \"#1f77b4\"]  # green, red, orange, blue\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.bar(labels_bar, counts, color=colors_bar)\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Alarm Outcomes – 10-Min Early Warning (Validation set)\")\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v, str(v), ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Confusion Matrix Plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Build confusion matrix in sklearn format:\n",
    "# [[TN, FP],\n",
    "#  [FN, TP]]\n",
    "cm = np.array([[tn, fp],\n",
    "               [fn, tp]])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "im = ax.imshow(cm, cmap=\"Blues\")\n",
    "\n",
    "# Annotate each cell with its numeric value\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", fontsize=12, color=\"black\")\n",
    "\n",
    "# Labels + ticks\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels([\"Predicted No Alarm\", \"Predicted Alarm\"])\n",
    "ax.set_yticklabels([\"Actual No Alarm\", \"Actual Alarm\"])\n",
    "\n",
    "ax.set_title(\"Confusion Matrix\", fontsize=14)\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13999fa4-973f-4dbe-b2d1-e4fafbbf9c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 9: Label Leakage Validation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fde4b80-2b9a-45c9-8679-b2037d7a4e4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ----- 1. Prepare data (already engineered) -----\n",
    "X = pdf[feature_cols].values\n",
    "y = pdf[\"y\"].values.astype(int)\n",
    "\n",
    "n = len(pdf)\n",
    "split_idx = int(0.7 * n)   # time-based split: first 70% train, last 30% val\n",
    "\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# class weight like before\n",
    "pos = y_train.sum()\n",
    "neg = len(y_train) - pos\n",
    "scale_pos_weight = neg / max(pos, 1)\n",
    "\n",
    "common_params = dict(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\",\n",
    ")\n",
    "\n",
    "# ----- 2. Baseline model (real labels) -----\n",
    "model_real = XGBClassifier(**common_params)\n",
    "model_real.fit(X_train, y_train)\n",
    "\n",
    "probs_real = model_real.predict_proba(X_val)[:, 1]\n",
    "auc_real = roc_auc_score(y_val, probs_real)\n",
    "print(f\"Baseline ROC AUC (real labels): {auc_real:.3f}\")\n",
    "\n",
    "# ----- 3. Leakage test: train on SHUFFLED labels -----\n",
    "y_train_shuffled = y_train.copy()\n",
    "np.random.shuffle(y_train_shuffled)\n",
    "\n",
    "model_shuf = XGBClassifier(**common_params)\n",
    "model_shuf.fit(X_train, y_train_shuffled)\n",
    "\n",
    "probs_shuf = model_shuf.predict_proba(X_val)[:, 1]\n",
    "auc_shuf = roc_auc_score(y_val, probs_shuf)\n",
    "print(f\"Leakage test ROC AUC (train on shuffled labels): {auc_shuf:.3f}\")\n",
    "\n",
    "# Interpretation:\n",
    "# - auc_real high (e.g. 0.98–0.99) AND\n",
    "# - auc_shuf ~ 0.50\n",
    "# → strong evidence that the model is learning real structure, not leakage.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Nonwoven_PCA_EarlyAlarm_Detection_ML_Architect",
   "widgets": {
    "debug_mode": {
     "currentValue": "yes",
     "nuid": "98902d49-2ab7-46db-891a-e58b6e929969",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "yes",
      "label": "Debug Mode",
      "name": "debug_mode",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "yes",
        "no"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "yes",
      "label": "Debug Mode",
      "name": "debug_mode",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "yes",
        "no"
       ]
      }
     }
    },
    "end_date": {
     "currentValue": "2025-08-08",
     "nuid": "6276081c-8c6f-4553-9bce-0ae2e4aac8e9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2026-01-10",
      "label": null,
      "name": "end_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2026-01-10",
      "label": null,
      "name": "end_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "historian_name": {
     "currentValue": "CR008",
     "nuid": "d3f2088c-b334-4184-8a0b-fd7c590419a8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "CR008",
      "label": null,
      "name": "historian_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "CR008",
      "label": null,
      "name": "historian_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "pl_ids": {
     "currentValue": "103",
     "nuid": "a62f71c6-59f4-49b3-af28-ce944c1e1b70",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "103",
      "label": "PLID",
      "name": "pl_ids",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "103",
      "label": "PLID",
      "name": "pl_ids",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "pu_ids_base": {
     "currentValue": "1647",
     "nuid": "e32995ea-a6bc-489e-9bd6-ca9dcd8ac69a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1647",
      "label": "PUID Base",
      "name": "pu_ids_base",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1647",
      "label": "PUID Base",
      "name": "pu_ids_base",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "site_id": {
     "currentValue": "11",
     "nuid": "b6410e33-a41f-4731-ae41-65b0eabe2a7d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "11",
      "label": "Site iODS ID",
      "name": "site_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "11",
      "label": "Site iODS ID",
      "name": "site_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "site_name": {
     "currentValue": "CRA",
     "nuid": "2f574aa1-3a1f-498c-88ce-10cfb7f07939",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "CRA",
      "label": null,
      "name": "site_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "CRA",
      "label": null,
      "name": "site_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "site_server": {
     "currentValue": "CRA-PROF-DATA",
     "nuid": "bb060546-904a-40de-aed5-46718f93faa3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "CRA-PROF-DATA",
      "label": "Site iODS Server",
      "name": "site_server",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "CRA-PROF-DATA",
      "label": "Site iODS Server",
      "name": "site_server",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "start_date": {
     "currentValue": "2025-07-01",
     "nuid": "6962838b-fcd3-477c-985f-3bf081223998",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-11-21",
      "label": null,
      "name": "start_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2025-11-21",
      "label": null,
      "name": "start_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}